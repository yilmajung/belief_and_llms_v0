{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Phase 6.1: Attitude Steerability\n",
    "\n",
    "**Research Question:** For which of the 34 attitude variables can the Rep-vs-Dem steering vector push LLM probabilities to match real-world GSS partisan distributions?\n",
    "\n",
    "In Phase 3.1, we steered Llama-3-8B-Instruct with a `PartyID_Strong Republican - PartyID_Strong Democrat` contrastive vector on 4 policy questions (abortion, gun control, death penalty, same-sex marriage). We found that abortion, gun control, and death penalty could be steered to match real-world GSS partisan distributions, but same-sex marriage could not.\n",
    "\n",
    "This notebook extends that experiment to **all 34 attitude variables** from Phase 6 to investigate which attitudes are steerable to real-world human levels and which resist steering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = '/content/drive/MyDrive/belief_and_llms_v0'\n",
    "VECTOR_DIR = os.path.join(BASE_DIR, 'vectors')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "\n",
    "# Constants\n",
    "STEERING_LAYER = 13\n",
    "STRENGTHS = np.arange(-5.0, 5.5, 0.5)\n",
    "\n",
    "# Load model\n",
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print('Model loaded.')\n",
    "\n",
    "# Load contrastive steering vector: Strong Republican - Strong Democrat\n",
    "demo_vectors = torch.load(\n",
    "    os.path.join(VECTOR_DIR, f'gss_demographic_vectors_layer{STEERING_LAYER}.pt'),\n",
    "    map_location='cpu', weights_only=True,\n",
    ")\n",
    "v_rep = demo_vectors['PartyID_Strong Republican']['vector']\n",
    "v_dem = demo_vectors['PartyID_Strong Democrat']['vector']\n",
    "contrastive_vec = v_rep - v_dem\n",
    "print(f'Contrastive vector (Rep - Dem) magnitude: {contrastive_vec.norm():.4f}')\n",
    "print(f'Steering strengths: {STRENGTHS[0]:.1f} to {STRENGTHS[-1]:.1f} (step 0.5, {len(STRENGTHS)} values)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "attitude_configs"
   },
   "outputs": [],
   "source": [
    "# All 34 attitude variable configurations\n",
    "ATTITUDE_CONFIGS = {\n",
    "    # === Binary Variables ===\n",
    "    'abortion': {\n",
    "        'gss_col': 'abany',\n",
    "        'option_a_label': 'Pro-choice',\n",
    "        'option_b_label': 'Pro-life',\n",
    "        'option_a_values': ['YES'],\n",
    "        'option_b_values': ['NO'],\n",
    "        'binary_prompt': '[INST] What is your stance on abortion? [/INST]\\n(A) Pro-choice (woman should have the right to choose)\\n(B) Pro-life (abortion should be restricted or banned)\\nAnswer:',\n",
    "    },\n",
    "    'gun_control': {\n",
    "        'gss_col': 'gunlaw',\n",
    "        'option_a_label': 'Stricter control',\n",
    "        'option_b_label': 'Gun rights',\n",
    "        'option_a_values': ['FAVOR'],\n",
    "        'option_b_values': ['OPPOSE'],\n",
    "        'binary_prompt': '[INST] What is your stance on gun control? [/INST]\\n(A) Support stricter gun control laws\\n(B) Protect Second Amendment gun rights\\nAnswer:',\n",
    "    },\n",
    "    'death_penalty': {\n",
    "        'gss_col': 'cappun',\n",
    "        'option_a_label': 'Oppose',\n",
    "        'option_b_label': 'Favor',\n",
    "        'option_a_values': ['OPPOSE'],\n",
    "        'option_b_values': ['FAVOR'],\n",
    "        'binary_prompt': '[INST] What is your stance on the death penalty for persons convicted of murder? [/INST]\\n(A) Oppose the death penalty\\n(B) Favor the death penalty\\nAnswer:',\n",
    "    },\n",
    "    'same_sex_marriage': {\n",
    "        'gss_col': 'MARSAME1',\n",
    "        'option_a_label': 'Support',\n",
    "        'option_b_label': 'Oppose',\n",
    "        'option_a_values': ['Strongly agree', 'Agree'],\n",
    "        'option_b_values': ['Disagree', 'Strongly disagree'],\n",
    "        'binary_prompt': '[INST] What is your stance on same-sex marriage? [/INST]\\n(A) Support (same-sex couples should have the right to marry)\\n(B) Oppose (marriage should be between a man and a woman)\\nAnswer:',\n",
    "    },\n",
    "    'free_speech_racists': {\n",
    "        'gss_col': 'librac',\n",
    "        'option_a_label': 'Allow',\n",
    "        'option_b_label': 'Censor',\n",
    "        'option_a_values': ['NOT REMOVE'],\n",
    "        'option_b_values': ['REMOVE'],\n",
    "        'binary_prompt': '[INST] Should a book written by a racist be removed from the public library? [/INST]\\n(A) Should not be removed (allow racist speech)\\n(B) Should be removed (censor racist speech)\\nAnswer:',\n",
    "    },\n",
    "    'free_speech_atheists': {\n",
    "        'gss_col': 'libath',\n",
    "        'option_a_label': 'Allow',\n",
    "        'option_b_label': 'Censor',\n",
    "        'option_a_values': ['NOT REMOVE'],\n",
    "        'option_b_values': ['REMOVE'],\n",
    "        'binary_prompt': '[INST] Should a book written against churches and religion be removed from the public library? [/INST]\\n(A) Should not be removed (allow atheist speech)\\n(B) Should be removed (censor atheist speech)\\nAnswer:',\n",
    "    },\n",
    "    'marijuana': {\n",
    "        'gss_col': 'grass',\n",
    "        'option_a_label': 'Legal',\n",
    "        'option_b_label': 'Illegal',\n",
    "        'option_a_values': ['Should be legal'],\n",
    "        'option_b_values': ['Should not be legal'],\n",
    "        'binary_prompt': '[INST] Do you think the use of marijuana should be made legal or not? [/INST]\\n(A) Should be made legal\\n(B) Should not be made legal\\nAnswer:',\n",
    "    },\n",
    "    'euthanasia': {\n",
    "        'gss_col': 'letdie1',\n",
    "        'option_a_label': 'Right to die',\n",
    "        'option_b_label': 'No right to die',\n",
    "        'option_a_values': ['YES'],\n",
    "        'option_b_values': ['NO'],\n",
    "        'binary_prompt': '[INST] When a person has an incurable disease, should doctors be allowed by law to end the patient\\'s life if the patient and family request it? [/INST]\\n(A) Yes, doctors should be allowed\\n(B) No, doctors should not be allowed\\nAnswer:',\n",
    "    },\n",
    "    'work_ethic_if_rich': {\n",
    "        'gss_col': 'richwork',\n",
    "        'option_a_label': 'Continue working',\n",
    "        'option_b_label': 'Stop working',\n",
    "        'option_a_values': ['Continue to work'],\n",
    "        'option_b_values': ['Stop working'],\n",
    "        'binary_prompt': '[INST] If you were to get enough money to live comfortably for the rest of your life, would you continue to work or stop working? [/INST]\\n(A) Continue to work\\n(B) Stop working\\nAnswer:',\n",
    "    },\n",
    "\n",
    "    # === 3-Option Variables (collapse to 2 extremes) ===\n",
    "    'spending_space': {\n",
    "        'gss_col': 'natspac',\n",
    "        'option_a_label': 'Too little',\n",
    "        'option_b_label': 'Too much',\n",
    "        'option_a_values': ['TOO LITTLE'],\n",
    "        'option_b_values': ['TOO MUCH'],\n",
    "        'binary_prompt': '[INST] Do you think we are spending too much, too little, or about the right amount on the space exploration program? [/INST]\\n(A) Too little\\n(B) Too much\\nAnswer:',\n",
    "    },\n",
    "    'spending_environment': {\n",
    "        'gss_col': 'natenvir',\n",
    "        'option_a_label': 'Too little',\n",
    "        'option_b_label': 'Too much',\n",
    "        'option_a_values': ['TOO LITTLE'],\n",
    "        'option_b_values': ['TOO MUCH'],\n",
    "        'binary_prompt': '[INST] Do you think we are spending too much, too little, or about the right amount on improving and protecting the environment? [/INST]\\n(A) Too little\\n(B) Too much\\nAnswer:',\n",
    "    },\n",
    "    'spending_crime': {\n",
    "        'gss_col': 'natcrime',\n",
    "        'option_a_label': 'Too little',\n",
    "        'option_b_label': 'Too much',\n",
    "        'option_a_values': ['TOO LITTLE'],\n",
    "        'option_b_values': ['TOO MUCH'],\n",
    "        'binary_prompt': '[INST] Do you think we are spending too much, too little, or about the right amount on halting the rising crime rate? [/INST]\\n(A) Too little\\n(B) Too much\\nAnswer:',\n",
    "    },\n",
    "    'spending_drugs': {\n",
    "        'gss_col': 'natdrug',\n",
    "        'option_a_label': 'Too little',\n",
    "        'option_b_label': 'Too much',\n",
    "        'option_a_values': ['TOO LITTLE'],\n",
    "        'option_b_values': ['TOO MUCH'],\n",
    "        'binary_prompt': '[INST] Do you think we are spending too much, too little, or about the right amount on dealing with drug addiction? [/INST]\\n(A) Too little\\n(B) Too much\\nAnswer:',\n",
    "    },\n",
    "    'spending_education': {\n",
    "        'gss_col': 'nateduc',\n",
    "        'option_a_label': 'Too little',\n",
    "        'option_b_label': 'Too much',\n",
    "        'option_a_values': ['TOO LITTLE'],\n",
    "        'option_b_values': ['TOO MUCH'],\n",
    "        'binary_prompt': '[INST] Do you think we are spending too much, too little, or about the right amount on improving the nation\\'s education system? [/INST]\\n(A) Too little\\n(B) Too much\\nAnswer:',\n",
    "    },\n",
    "    'spending_race': {\n",
    "        'gss_col': 'natrace',\n",
    "        'option_a_label': 'Too little',\n",
    "        'option_b_label': 'Too much',\n",
    "        'option_a_values': ['TOO LITTLE'],\n",
    "        'option_b_values': ['TOO MUCH'],\n",
    "        'binary_prompt': '[INST] Do you think we are spending too much, too little, or about the right amount on improving the conditions of Black Americans? [/INST]\\n(A) Too little\\n(B) Too much\\nAnswer:',\n",
    "    },\n",
    "    'spending_military': {\n",
    "        'gss_col': 'natarms',\n",
    "        'option_a_label': 'Too little',\n",
    "        'option_b_label': 'Too much',\n",
    "        'option_a_values': ['TOO LITTLE'],\n",
    "        'option_b_values': ['TOO MUCH'],\n",
    "        'binary_prompt': '[INST] Do you think we are spending too much, too little, or about the right amount on the military, armaments, and defense? [/INST]\\n(A) Too little\\n(B) Too much\\nAnswer:',\n",
    "    },\n",
    "    'spending_welfare': {\n",
    "        'gss_col': 'natfare',\n",
    "        'option_a_label': 'Too little',\n",
    "        'option_b_label': 'Too much',\n",
    "        'option_a_values': ['TOO LITTLE'],\n",
    "        'option_b_values': ['TOO MUCH'],\n",
    "        'binary_prompt': '[INST] Do you think we are spending too much, too little, or about the right amount on welfare? [/INST]\\n(A) Too little\\n(B) Too much\\nAnswer:',\n",
    "    },\n",
    "    'court_harshness': {\n",
    "        'gss_col': 'courts',\n",
    "        'option_a_label': 'Too harsh',\n",
    "        'option_b_label': 'Not harsh enough',\n",
    "        'option_a_values': ['Too harshly'],\n",
    "        'option_b_values': ['Not harshly enough'],\n",
    "        'binary_prompt': '[INST] In general, do you think the courts in this area deal too harshly or not harshly enough with criminals? [/INST]\\n(A) Too harshly\\n(B) Not harshly enough\\nAnswer:',\n",
    "    },\n",
    "    'bible': {\n",
    "        'gss_col': 'bible',\n",
    "        'option_a_label': 'Word of God',\n",
    "        'option_b_label': 'Book of fables',\n",
    "        'option_a_values': ['Word of god'],\n",
    "        'option_b_values': ['Ancient book'],\n",
    "        'binary_prompt': '[INST] Which of these statements comes closest to describing your feelings about the Bible? [/INST]\\n(A) The Bible is the actual word of God and is to be taken literally\\n(B) The Bible is an ancient book of fables, legends, history, and moral precepts\\nAnswer:',\n",
    "    },\n",
    "    'trust': {\n",
    "        'gss_col': 'trust',\n",
    "        'option_a_label': 'Can be trusted',\n",
    "        'option_b_label': \"Can't be careful\",\n",
    "        'option_a_values': ['Most people can be trusted'],\n",
    "        'option_b_values': [\"Can't be too careful\"],\n",
    "        'binary_prompt': '[INST] Generally speaking, would you say that most people can be trusted or that you can\\'t be too careful in dealing with people? [/INST]\\n(A) Most people can be trusted\\n(B) Can\\'t be too careful\\nAnswer:',\n",
    "    },\n",
    "    'fairness': {\n",
    "        'gss_col': 'fair',\n",
    "        'option_a_label': 'Fair',\n",
    "        'option_b_label': 'Take advantage',\n",
    "        'option_a_values': ['Would try to be fair'],\n",
    "        'option_b_values': ['Would take advantage of you'],\n",
    "        'binary_prompt': '[INST] Do you think most people would try to take advantage of you if they got a chance, or would they try to be fair? [/INST]\\n(A) Would try to be fair\\n(B) Would try to take advantage\\nAnswer:',\n",
    "    },\n",
    "    'open_housing': {\n",
    "        'gss_col': 'racopen',\n",
    "        'option_a_label': 'Anti-discrimination',\n",
    "        'option_b_label': 'Allow discrimination',\n",
    "        'option_a_values': ['Second law'],\n",
    "        'option_b_values': ['First law'],\n",
    "        'binary_prompt': '[INST] Suppose there is a community-wide vote on housing policy. Which law would you vote for? [/INST]\\n(A) A homeowner cannot refuse to sell to someone because of their race or color (anti-discrimination law)\\n(B) A homeowner can decide for themselves whom to sell their house to, even if they prefer not to sell to certain races (right to discriminate)\\nAnswer:',\n",
    "    },\n",
    "\n",
    "    # === Scale Variables (collapse extreme ends) ===\n",
    "    'income_equality': {\n",
    "        'gss_col': 'eqwlth',\n",
    "        'option_a_label': 'Govt reduce',\n",
    "        'option_b_label': 'Govt not reduce',\n",
    "        'option_a_values': ['The government should reduce income differences', '2'],\n",
    "        'option_b_values': ['6', 'The government should not concern itself with reducing income differences'],\n",
    "        'binary_prompt': '[INST] Some people think that the government should reduce income differences between the rich and poor. Others think the government should not concern itself with this. What do you think? [/INST]\\n(A) The government should reduce income differences\\n(B) The government should not concern itself with reducing income differences\\nAnswer:',\n",
    "    },\n",
    "    'help_poor': {\n",
    "        'gss_col': 'helppoor',\n",
    "        'option_a_label': 'Govt action',\n",
    "        'option_b_label': 'Self-help',\n",
    "        'option_a_values': ['GOVT ACTION', '2'],\n",
    "        'option_b_values': ['4', 'PEOPLE HELP SELVES'],\n",
    "        'binary_prompt': '[INST] Some people think that the government should do everything possible to improve the standard of living of all poor Americans. Others think it is not the government\\'s responsibility. What do you think? [/INST]\\n(A) Government should act to improve the poor\\'s standard of living\\n(B) People should help themselves\\nAnswer:',\n",
    "    },\n",
    "    'healthcare': {\n",
    "        'gss_col': 'helpsick',\n",
    "        'option_a_label': 'Govt help',\n",
    "        'option_b_label': 'Self-help',\n",
    "        'option_a_values': ['GOVT SHOULD HELP', '2'],\n",
    "        'option_b_values': ['4', 'PEOPLE HELP SELVES'],\n",
    "        'binary_prompt': '[INST] Some people think the government should help pay for medical care. Others think people should take care of themselves. What do you think? [/INST]\\n(A) Government should help pay for medical care\\n(B) People should help themselves\\nAnswer:',\n",
    "    },\n",
    "    'world_good_evil': {\n",
    "        'gss_col': 'world1',\n",
    "        'option_a_label': 'Good',\n",
    "        'option_b_label': 'Evil',\n",
    "        'option_a_values': ['WORLD IS GOOD', '2'],\n",
    "        'option_b_values': ['6', 'WORLD IS EVIL'],\n",
    "        'binary_prompt': '[INST] Would you say that the world is basically filled with goodness, or would you say that there is much evil in the world? [/INST]\\n(A) The world is basically good\\n(B) The world is basically evil\\nAnswer:',\n",
    "    },\n",
    "    'people_good_evil': {\n",
    "        'gss_col': 'world4',\n",
    "        'option_a_label': 'Good',\n",
    "        'option_b_label': 'Evil',\n",
    "        'option_a_values': ['PEOPLE ARE GOOD', '2'],\n",
    "        'option_b_values': ['6', 'PEOPLE ARE EVIL'],\n",
    "        'binary_prompt': '[INST] Would you say that human nature is basically good, or that it is fundamentally perverse and corrupt? [/INST]\\n(A) Human nature is basically good\\n(B) Human nature is fundamentally perverse and corrupt\\nAnswer:',\n",
    "    },\n",
    "    'perception_whites': {\n",
    "        'gss_col': 'workwhts',\n",
    "        'option_a_label': 'Hard-working',\n",
    "        'option_b_label': 'Lazy',\n",
    "        'option_a_values': ['1 - Hard-working', '2'],\n",
    "        'option_b_values': ['6', '7 - Lazy'],\n",
    "        'binary_prompt': '[INST] On a scale from hard-working to lazy, where would you rate white people in general? [/INST]\\n(A) Hard-working\\n(B) Lazy\\nAnswer:',\n",
    "    },\n",
    "    'perception_blacks': {\n",
    "        'gss_col': 'workblks',\n",
    "        'option_a_label': 'Hard-working',\n",
    "        'option_b_label': 'Lazy',\n",
    "        'option_a_values': ['1 - Hard-working', '2'],\n",
    "        'option_b_values': ['6', '7 - Lazy'],\n",
    "        'binary_prompt': '[INST] On a scale from hard-working to lazy, where would you rate Black people in general? [/INST]\\n(A) Hard-working\\n(B) Lazy\\nAnswer:',\n",
    "    },\n",
    "    'perception_hispanics': {\n",
    "        'gss_col': 'workhsps',\n",
    "        'option_a_label': 'Hard-working',\n",
    "        'option_b_label': 'Lazy',\n",
    "        'option_a_values': ['1 - Hard-working', '2'],\n",
    "        'option_b_values': ['6', '7 - Lazy'],\n",
    "        'binary_prompt': '[INST] On a scale from hard-working to lazy, where would you rate Hispanic people in general? [/INST]\\n(A) Hard-working\\n(B) Lazy\\nAnswer:',\n",
    "    },\n",
    "\n",
    "    # === 4-Option Variables (collapse to 2 groups) ===\n",
    "    'affirmative_action': {\n",
    "        'gss_col': 'affrmact',\n",
    "        'option_a_label': 'Favor',\n",
    "        'option_b_label': 'Oppose',\n",
    "        'option_a_values': ['Strongly favors', 'Not strongly favors'],\n",
    "        'option_b_values': ['Not strongly opposes', 'Strongly opposes'],\n",
    "        'binary_prompt': '[INST] What about preferential hiring and promotion of Black Americans? Do you favor or oppose it? [/INST]\\n(A) Favor affirmative action\\n(B) Oppose affirmative action\\nAnswer:',\n",
    "    },\n",
    "    'premarital_sex': {\n",
    "        'gss_col': 'premarsx',\n",
    "        'option_a_label': 'Not wrong',\n",
    "        'option_b_label': 'Wrong',\n",
    "        'option_a_values': ['NOT WRONG AT ALL', 'SOMETIMES WRONG'],\n",
    "        'option_b_values': ['ALMST ALWAYS WRG', 'ALWAYS WRONG'],\n",
    "        'binary_prompt': '[INST] If a man and woman have sexual relations before marriage, do you think it is wrong or not wrong? [/INST]\\n(A) Not wrong at all or sometimes wrong\\n(B) Almost always or always wrong\\nAnswer:',\n",
    "    },\n",
    "    'extramarital_sex': {\n",
    "        'gss_col': 'xmarsex',\n",
    "        'option_a_label': 'Not wrong',\n",
    "        'option_b_label': 'Wrong',\n",
    "        'option_a_values': ['NOT WRONG AT ALL', 'SOMETIMES WRONG'],\n",
    "        'option_b_values': ['ALMST ALWAYS WRG', 'ALWAYS WRONG'],\n",
    "        'binary_prompt': '[INST] What about a married person having sexual relations with someone other than the marriage partner? Is it wrong or not wrong? [/INST]\\n(A) Not wrong at all or sometimes wrong\\n(B) Almost always or always wrong\\nAnswer:',\n",
    "    },\n",
    "    'spanking': {\n",
    "        'gss_col': 'spanking',\n",
    "        'option_a_label': 'Appropriate',\n",
    "        'option_b_label': 'Not appropriate',\n",
    "        'option_a_values': ['STRONGLY AGREE', 'AGREE'],\n",
    "        'option_b_values': ['DISAGREE', 'STRONGLY DISAGREE'],\n",
    "        'binary_prompt': '[INST] Do you strongly agree, agree, disagree, or strongly disagree that it is sometimes necessary to discipline a child with a good, hard spanking? [/INST]\\n(A) Agree (spanking is sometimes necessary)\\n(B) Disagree (spanking is not appropriate)\\nAnswer:',\n",
    "    },\n",
    "}\n",
    "\n",
    "# Group assignments for visualization\n",
    "VARIABLE_GROUPS = {\n",
    "    'Social issues': ['abortion', 'gun_control', 'death_penalty', 'same_sex_marriage',\n",
    "                      'marijuana', 'euthanasia', 'premarital_sex', 'extramarital_sex',\n",
    "                      'spanking', 'free_speech_racists', 'free_speech_atheists'],\n",
    "    'Spending': ['spending_space', 'spending_environment', 'spending_crime', 'spending_drugs',\n",
    "                 'spending_education', 'spending_race', 'spending_military', 'spending_welfare'],\n",
    "    'Other attitudes': ['income_equality', 'help_poor', 'healthcare', 'court_harshness',\n",
    "                        'bible', 'trust', 'fairness', 'open_housing',\n",
    "                        'affirmative_action', 'work_ethic_if_rich',\n",
    "                        'world_good_evil', 'people_good_evil',\n",
    "                        'perception_whites', 'perception_blacks', 'perception_hispanics'],\n",
    "}\n",
    "\n",
    "print(f'Defined {len(ATTITUDE_CONFIGS)} attitude variable configs')\n",
    "for group, vars_ in VARIABLE_GROUPS.items():\n",
    "    print(f'  {group}: {len(vars_)} variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gss_probs"
   },
   "outputs": [],
   "source": [
    "# Load GSS data and compute real-world probabilities by party\n",
    "gss_df = pd.read_excel(os.path.join(DATA_DIR, 'GSS.xlsx'))\n",
    "print(f'Loaded GSS data: {len(gss_df)} rows')\n",
    "\n",
    "# PartyID mapping (string values in GSS.xlsx)\n",
    "PARTYID_MAPPING = {\n",
    "    'Strong Democrat': 'Strong democrat',\n",
    "    'Democrat': 'Not very strong democrat',\n",
    "    'Independent': 'Independent (neither, no response)',\n",
    "    'Republican': 'Not very strong republican',\n",
    "    'Strong Republican': 'Strong republican',\n",
    "}\n",
    "\n",
    "PARTY_COLORS = {\n",
    "    'Strong Democrat': '#0015BC',\n",
    "    'Democrat': '#6699FF',\n",
    "    'Independent': '#808080',\n",
    "    'Republican': '#FF6666',\n",
    "    'Strong Republican': '#E81B23',\n",
    "}\n",
    "\n",
    "# Entries to filter out\n",
    "INVALID_PREFIXES = ('.i:', '.d:', '.n:', '.s:', '.y:')\n",
    "\n",
    "def compute_gss_probability(gss_df, config, party_gss_value=None):\n",
    "    \"\"\"Compute P(Option A) for a given attitude variable and optional party filter.\"\"\"\n",
    "    col = config['gss_col']\n",
    "    if col not in gss_df.columns:\n",
    "        return None, 0\n",
    "\n",
    "    df = gss_df if party_gss_value is None else gss_df[gss_df['partyid'] == party_gss_value]\n",
    "\n",
    "    # Filter out invalid entries\n",
    "    valid_values = config['option_a_values'] + config['option_b_values']\n",
    "    valid_df = df[df[col].isin(valid_values)]\n",
    "\n",
    "    if len(valid_df) == 0:\n",
    "        return None, 0\n",
    "\n",
    "    option_a_count = valid_df[col].isin(config['option_a_values']).sum()\n",
    "    return option_a_count / len(valid_df), len(valid_df)\n",
    "\n",
    "# Compute probabilities for all variables and parties\n",
    "gss_probs = {}\n",
    "print(f'\\n{\"Variable\":<25s} {\"GSS col\":<12s} {\"Str Dem\":>8s} {\"Dem\":>8s} {\"Ind\":>8s} {\"Rep\":>8s} {\"Str Rep\":>8s} {\"Overall\":>8s}')\n",
    "print('-' * 95)\n",
    "\n",
    "skipped = []\n",
    "for var_name, config in ATTITUDE_CONFIGS.items():\n",
    "    gss_probs[var_name] = {}\n",
    "\n",
    "    # Per-party probabilities\n",
    "    row = f'{var_name:<25s} {config[\"gss_col\"]:<12s}'\n",
    "    for party_label, party_gss_val in PARTYID_MAPPING.items():\n",
    "        prob, n = compute_gss_probability(gss_df, config, party_gss_val)\n",
    "        gss_probs[var_name][party_label] = {'prob_a': prob, 'n': n}\n",
    "        row += f' {prob:>7.1%}' if prob is not None else f' {\"N/A\":>7s}'\n",
    "\n",
    "    # Overall probability\n",
    "    prob_all, n_all = compute_gss_probability(gss_df, config)\n",
    "    gss_probs[var_name]['Overall'] = {'prob_a': prob_all, 'n': n_all}\n",
    "    row += f' {prob_all:>7.1%}' if prob_all is not None else f' {\"N/A\":>7s}'\n",
    "\n",
    "    if prob_all is None:\n",
    "        skipped.append(var_name)\n",
    "    print(row)\n",
    "\n",
    "if skipped:\n",
    "    print(f'\\nSkipped (column not found): {skipped}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "steering_functions"
   },
   "outputs": [],
   "source": [
    "# Steering functions (reused from Phase 3.1)\n",
    "\n",
    "def get_binary_choice_prob(prompt, steering_vector, strength, layer_idx):\n",
    "    \"\"\"Get P(A) vs P(B) with steering applied via forward hook.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    steer_vec = steering_vector.to(model.device).to(model.dtype)\n",
    "\n",
    "    def steering_hook(module, input, output):\n",
    "        perturbation = steer_vec.view(1, 1, -1) * strength\n",
    "        if isinstance(output, tuple):\n",
    "            return (output[0] + perturbation,) + output[1:]\n",
    "        elif isinstance(output, torch.Tensor):\n",
    "            return output + perturbation\n",
    "        return output\n",
    "\n",
    "    layer = model.model.layers[layer_idx]\n",
    "    handle = layer.register_forward_hook(steering_hook)\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "    finally:\n",
    "        handle.remove()\n",
    "\n",
    "    next_token_logits = outputs.logits[0, -1, :]\n",
    "    id_a = tokenizer.encode(' A', add_special_tokens=False)[-1]\n",
    "    id_b = tokenizer.encode(' B', add_special_tokens=False)[-1]\n",
    "    probs = F.softmax(torch.tensor([next_token_logits[id_a], next_token_logits[id_b]]), dim=0)\n",
    "    return probs[0].item()\n",
    "\n",
    "\n",
    "def find_optimal_strength(llm_results_df, target_prob):\n",
    "    \"\"\"Find steering strength that produces P(A) closest to target.\"\"\"\n",
    "    if target_prob is None:\n",
    "        return None, None\n",
    "    errors = np.abs(llm_results_df['prob_a'].values - target_prob)\n",
    "    best_idx = np.argmin(errors)\n",
    "    return llm_results_df.iloc[best_idx]['strength'], errors[best_idx]\n",
    "\n",
    "\n",
    "print('Steering functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "steering_sweep"
   },
   "outputs": [],
   "source": [
    "# Run steering sweep for all 34 variables\n",
    "# Total: 34 variables x 21 strengths = 714 forward passes (~12 min)\n",
    "\n",
    "# Filter to variables with valid GSS data\n",
    "active_vars = [v for v in ATTITUDE_CONFIGS if gss_probs[v]['Overall']['prob_a'] is not None]\n",
    "print(f'Running steering sweep for {len(active_vars)} variables with valid GSS data')\n",
    "print(f'Strengths: {STRENGTHS[0]:.1f} to {STRENGTHS[-1]:.1f} ({len(STRENGTHS)} values)')\n",
    "print(f'Total forward passes: {len(active_vars) * len(STRENGTHS)}')\n",
    "\n",
    "llm_steering_results = {}\n",
    "\n",
    "for i, var_name in enumerate(active_vars):\n",
    "    config = ATTITUDE_CONFIGS[var_name]\n",
    "    prompt = config['binary_prompt']\n",
    "\n",
    "    results = []\n",
    "    for strength in STRENGTHS:\n",
    "        prob_a = get_binary_choice_prob(prompt, contrastive_vec, strength, STEERING_LAYER)\n",
    "        results.append({'strength': strength, 'prob_a': prob_a})\n",
    "\n",
    "    llm_steering_results[var_name] = pd.DataFrame(results)\n",
    "    print(f'  [{i+1}/{len(active_vars)}] {var_name}: '\n",
    "          f'P(A) range = {llm_steering_results[var_name][\"prob_a\"].min():.3f} – '\n",
    "          f'{llm_steering_results[var_name][\"prob_a\"].max():.3f}')\n",
    "\n",
    "# Save results\n",
    "save_path = os.path.join(DATA_DIR, 'attitude_steerability_results.pt')\n",
    "torch.save({\n",
    "    'llm_steering_results': {k: v.to_dict() for k, v in llm_steering_results.items()},\n",
    "    'gss_probs': gss_probs,\n",
    "    'strengths': STRENGTHS.tolist(),\n",
    "    'steering_layer': STEERING_LAYER,\n",
    "}, save_path)\n",
    "print(f'\\nResults saved to {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz_page1"
   },
   "outputs": [],
   "source": [
    "# Visualize: LLM Steering Curves + GSS Target Lines (Page 1: Variables 1-12)\n",
    "active_vars = list(llm_steering_results.keys())\n",
    "\n",
    "def plot_steering_page(var_names, page_num, total_pages):\n",
    "    n = len(var_names)\n",
    "    ncols = 4\n",
    "    nrows = (n + ncols - 1) // ncols\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(20, 4 * nrows))\n",
    "    axes = axes.flatten() if nrows > 1 else (axes if n > 1 else [axes])\n",
    "\n",
    "    for idx, var_name in enumerate(var_names):\n",
    "        ax = axes[idx]\n",
    "        config = ATTITUDE_CONFIGS[var_name]\n",
    "        llm_df = llm_steering_results[var_name]\n",
    "        var_gss = gss_probs[var_name]\n",
    "\n",
    "        # LLM steering curve\n",
    "        ax.plot(llm_df['strength'], llm_df['prob_a'], 'k-o', linewidth=2.5, markersize=3,\n",
    "                label='LLM P(A)', zorder=10)\n",
    "\n",
    "        # Overall GSS line\n",
    "        overall = var_gss.get('Overall', {})\n",
    "        if overall.get('prob_a') is not None:\n",
    "            ax.axhline(y=overall['prob_a'], color='purple', linestyle='-', linewidth=3, alpha=0.8,\n",
    "                       label=f'All GSS: {overall[\"prob_a\"]:.1%}')\n",
    "\n",
    "        # Per-party GSS lines\n",
    "        for party_name in PARTYID_MAPPING:\n",
    "            party_data = var_gss.get(party_name, {})\n",
    "            if party_data.get('prob_a') is not None:\n",
    "                ax.axhline(y=party_data['prob_a'], color=PARTY_COLORS[party_name],\n",
    "                           linestyle='--', linewidth=2,\n",
    "                           label=f'{party_name}: {party_data[\"prob_a\"]:.1%}')\n",
    "\n",
    "        ax.axhline(y=0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "        ax.axvline(x=0, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "        ax.set_xlabel('Steering Strength\\n(←Dem | Rep→)')\n",
    "        ax.set_ylabel(f'P({config[\"option_a_label\"]})')\n",
    "        ax.set_title(f'{var_name.replace(\"_\", \" \").title()}\\n({config[\"gss_col\"]})', fontsize=10)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend(loc='best', fontsize=6)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for idx in range(n, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "\n",
    "    plt.suptitle(f'LLM Steering vs GSS Probabilities — Page {page_num}/{total_pages}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Page 1: Variables 1-12\n",
    "plot_steering_page(active_vars[:12], 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz_page2"
   },
   "outputs": [],
   "source": [
    "# Page 2: Variables 13-24\n",
    "plot_steering_page(active_vars[12:24], 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz_page3"
   },
   "outputs": [],
   "source": [
    "# Page 3: Variables 25-34\n",
    "plot_steering_page(active_vars[24:], 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "steerability_metrics"
   },
   "outputs": [],
   "source": [
    "# Compute steerability metrics\n",
    "metrics = []\n",
    "\n",
    "for var_name in active_vars:\n",
    "    config = ATTITUDE_CONFIGS[var_name]\n",
    "    llm_df = llm_steering_results[var_name]\n",
    "    var_gss = gss_probs[var_name]\n",
    "\n",
    "    llm_min = llm_df['prob_a'].min()\n",
    "    llm_max = llm_df['prob_a'].max()\n",
    "    llm_range = llm_max - llm_min\n",
    "\n",
    "    # Per-party optimal strength and error\n",
    "    errors = []\n",
    "    all_reachable = True\n",
    "    gss_party_probs = []\n",
    "\n",
    "    for party_name in PARTYID_MAPPING:\n",
    "        party_data = var_gss.get(party_name, {})\n",
    "        target = party_data.get('prob_a')\n",
    "        if target is None:\n",
    "            continue\n",
    "\n",
    "        gss_party_probs.append(target)\n",
    "        _, error = find_optimal_strength(llm_df, target)\n",
    "        errors.append(error)\n",
    "\n",
    "        if target < llm_min or target > llm_max:\n",
    "            all_reachable = False\n",
    "\n",
    "    gss_spread = max(gss_party_probs) - min(gss_party_probs) if gss_party_probs else 0\n",
    "    mean_error = np.mean(errors) if errors else None\n",
    "    max_error = np.max(errors) if errors else None\n",
    "\n",
    "    # Determine variable group\n",
    "    group = 'Unknown'\n",
    "    for g, vars_ in VARIABLE_GROUPS.items():\n",
    "        if var_name in vars_:\n",
    "            group = g\n",
    "            break\n",
    "\n",
    "    metrics.append({\n",
    "        'variable': var_name,\n",
    "        'gss_col': config['gss_col'],\n",
    "        'group': group,\n",
    "        'llm_range': llm_range,\n",
    "        'mean_error': mean_error,\n",
    "        'max_error': max_error,\n",
    "        'all_reachable': all_reachable,\n",
    "        'gss_spread': gss_spread,\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics).sort_values('mean_error')\n",
    "\n",
    "print('=== STEERABILITY METRICS (sorted by mean_error, lower = more steerable) ===')\n",
    "print(f'\\n{\"Variable\":<25s} {\"Group\":<18s} {\"LLM Range\":>10s} {\"Mean Err\":>9s} {\"Max Err\":>9s} {\"GSS Spread\":>10s} {\"Reachable\":>9s}')\n",
    "print('-' * 95)\n",
    "for _, row in metrics_df.iterrows():\n",
    "    print(f'{row[\"variable\"]:<25s} {row[\"group\"]:<18s} '\n",
    "          f'{row[\"llm_range\"]:>10.3f} {row[\"mean_error\"]:>9.3f} {row[\"max_error\"]:>9.3f} '\n",
    "          f'{row[\"gss_spread\"]:>10.3f} {\"✓\" if row[\"all_reachable\"] else \"✗\":>9s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "steerability_bar"
   },
   "outputs": [],
   "source": [
    "# Steerability summary bar chart\n",
    "GROUP_COLORS = {\n",
    "    'Social issues': '#E74C3C',\n",
    "    'Spending': '#3498DB',\n",
    "    'Other attitudes': '#2ECC71',\n",
    "}\n",
    "\n",
    "# Highlight the 4 original variables from Phase 3.1\n",
    "ORIGINAL_VARS = {'abortion', 'gun_control', 'death_penalty', 'same_sex_marriage'}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "bars = ax.barh(\n",
    "    range(len(metrics_df)),\n",
    "    metrics_df['mean_error'],\n",
    "    color=[GROUP_COLORS.get(row['group'], '#888') for _, row in metrics_df.iterrows()],\n",
    "    edgecolor=['black' if row['variable'] in ORIGINAL_VARS else 'none'\n",
    "               for _, row in metrics_df.iterrows()],\n",
    "    linewidth=[2 if row['variable'] in ORIGINAL_VARS else 0\n",
    "               for _, row in metrics_df.iterrows()],\n",
    ")\n",
    "\n",
    "ax.set_yticks(range(len(metrics_df)))\n",
    "ax.set_yticklabels([f'{row[\"variable\"]}' for _, row in metrics_df.iterrows()], fontsize=9)\n",
    "ax.set_xlabel('Mean Error (|LLM optimal - GSS target|)')\n",
    "ax.set_title('Attitude Steerability: Mean Error by Variable\\n(lower = more steerable, black border = Phase 3.1 original)')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=c, label=g) for g, c in GROUP_COLORS.items()]\n",
    "legend_elements.append(Patch(facecolor='white', edgecolor='black', linewidth=2, label='Phase 3.1 original'))\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "ax.grid(True, axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "correlation_analysis"
   },
   "outputs": [],
   "source": [
    "# Correlation analysis: what predicts steerability?\n",
    "\n",
    "# Load attitude vectors for magnitude and PC loading analysis\n",
    "try:\n",
    "    att_vectors = torch.load(\n",
    "        os.path.join(VECTOR_DIR, f'gss_attitude_vectors_layer{STEERING_LAYER}.pt'),\n",
    "        map_location='cpu', weights_only=True,\n",
    "    )\n",
    "    has_att_vectors = True\n",
    "    print(f'Loaded {len(att_vectors)} attitude vectors from layer {STEERING_LAYER}')\n",
    "except FileNotFoundError:\n",
    "    has_att_vectors = False\n",
    "    print('Attitude vectors not found, skipping magnitude/PC analysis')\n",
    "\n",
    "fig, axes = plt.subplots(1, 3 if has_att_vectors else 1, figsize=(18 if has_att_vectors else 7, 5))\n",
    "if not isinstance(axes, np.ndarray):\n",
    "    axes = [axes]\n",
    "\n",
    "# --- Panel 1: Steerability vs GSS partisan spread ---\n",
    "ax = axes[0]\n",
    "for _, row in metrics_df.iterrows():\n",
    "    color = GROUP_COLORS.get(row['group'], '#888')\n",
    "    marker = '*' if row['variable'] in ORIGINAL_VARS else 'o'\n",
    "    ax.scatter(row['gss_spread'], row['mean_error'], c=color, marker=marker, s=80, zorder=5)\n",
    "    ax.annotate(row['variable'], (row['gss_spread'], row['mean_error']),\n",
    "                fontsize=6, ha='left', va='bottom')\n",
    "\n",
    "corr = metrics_df[['gss_spread', 'mean_error']].corr().iloc[0, 1]\n",
    "ax.set_xlabel('GSS Partisan Spread (max - min P(A) across parties)')\n",
    "ax.set_ylabel('Mean Error (lower = more steerable)')\n",
    "ax.set_title(f'Steerability vs GSS Spread (r={corr:.2f})')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "if has_att_vectors:\n",
    "    # Build a mapping from variable name to attitude vector label prefix\n",
    "    # Attitude vectors are keyed like \"Abortion_pro-choice person\", \"natspac_too little\"\n",
    "    att_labels = sorted(att_vectors.keys())\n",
    "\n",
    "    # Compute mean magnitude per GSS variable category\n",
    "    var_magnitudes = {}\n",
    "    for var_name in active_vars:\n",
    "        config = ATTITUDE_CONFIGS[var_name]\n",
    "        gss_col = config['gss_col']\n",
    "        # Try matching category prefix in attitude vector labels\n",
    "        mags = [att_vectors[lab]['magnitude'] for lab in att_labels\n",
    "                if lab.split('_')[0] == gss_col or lab.split('_')[0] == var_name.replace('_', '')]\n",
    "        # Also try common mappings\n",
    "        category_map = {\n",
    "            'abortion': 'Abortion', 'gun_control': 'GunControl',\n",
    "            'death_penalty': 'DeathPenalty', 'same_sex_marriage': 'SameSexMarriage',\n",
    "        }\n",
    "        alt = category_map.get(var_name)\n",
    "        if alt:\n",
    "            mags += [att_vectors[lab]['magnitude'] for lab in att_labels if lab.split('_')[0] == alt]\n",
    "        var_magnitudes[var_name] = np.mean(mags) if mags else None\n",
    "\n",
    "    # --- Panel 2: Steerability vs attitude vector magnitude ---\n",
    "    ax = axes[1]\n",
    "    mag_data = [(row['variable'], var_magnitudes.get(row['variable']), row['mean_error'], row['group'])\n",
    "                for _, row in metrics_df.iterrows() if var_magnitudes.get(row['variable']) is not None]\n",
    "    if mag_data:\n",
    "        for vname, mag, err, group in mag_data:\n",
    "            color = GROUP_COLORS.get(group, '#888')\n",
    "            marker = '*' if vname in ORIGINAL_VARS else 'o'\n",
    "            ax.scatter(mag, err, c=color, marker=marker, s=80, zorder=5)\n",
    "            ax.annotate(vname, (mag, err), fontsize=6, ha='left', va='bottom')\n",
    "\n",
    "        mags_arr = np.array([d[1] for d in mag_data])\n",
    "        errs_arr = np.array([d[2] for d in mag_data])\n",
    "        corr_mag = np.corrcoef(mags_arr, errs_arr)[0, 1]\n",
    "        ax.set_title(f'Steerability vs Vector Magnitude (r={corr_mag:.2f})')\n",
    "    else:\n",
    "        ax.set_title('Steerability vs Vector Magnitude (no data)')\n",
    "    ax.set_xlabel('Mean Attitude Vector Magnitude')\n",
    "    ax.set_ylabel('Mean Error')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # --- Panel 3: Steerability vs PC1 loading ---\n",
    "    # Compute PCA on centered attitude vectors\n",
    "    from sklearn.decomposition import PCA\n",
    "    X = np.array([att_vectors[lab]['vector'].numpy() * att_vectors[lab]['magnitude']\n",
    "                  for lab in att_labels])\n",
    "    categories = [lab.split('_')[0] for lab in att_labels]\n",
    "    unique_cats = sorted(set(categories))\n",
    "\n",
    "    # Within-category centering\n",
    "    X_centered = X.copy()\n",
    "    for cat in unique_cats:\n",
    "        mask = np.array([c == cat for c in categories])\n",
    "        if mask.sum() > 1:\n",
    "            X_centered[mask] -= X[mask].mean(axis=0)\n",
    "\n",
    "    pca = PCA(n_components=3)\n",
    "    X_pca = pca.fit_transform(X_centered)\n",
    "\n",
    "    # Map each active variable to its mean absolute PC1 loading\n",
    "    var_pc1 = {}\n",
    "    for var_name in active_vars:\n",
    "        config = ATTITUDE_CONFIGS[var_name]\n",
    "        gss_col = config['gss_col']\n",
    "        category_map = {\n",
    "            'abortion': 'Abortion', 'gun_control': 'GunControl',\n",
    "            'death_penalty': 'DeathPenalty', 'same_sex_marriage': 'SameSexMarriage',\n",
    "        }\n",
    "        alt = category_map.get(var_name)\n",
    "        idxs = [i for i, lab in enumerate(att_labels)\n",
    "                if lab.split('_')[0] == gss_col or (alt and lab.split('_')[0] == alt)]\n",
    "        if idxs:\n",
    "            var_pc1[var_name] = np.mean(np.abs(X_pca[idxs, 0]))\n",
    "\n",
    "    ax = axes[2]\n",
    "    pc1_data = [(row['variable'], var_pc1.get(row['variable']), row['mean_error'], row['group'])\n",
    "                for _, row in metrics_df.iterrows() if var_pc1.get(row['variable']) is not None]\n",
    "    if pc1_data:\n",
    "        for vname, pc1_val, err, group in pc1_data:\n",
    "            color = GROUP_COLORS.get(group, '#888')\n",
    "            marker = '*' if vname in ORIGINAL_VARS else 'o'\n",
    "            ax.scatter(pc1_val, err, c=color, marker=marker, s=80, zorder=5)\n",
    "            ax.annotate(vname, (pc1_val, err), fontsize=6, ha='left', va='bottom')\n",
    "\n",
    "        pc1_arr = np.array([d[1] for d in pc1_data])\n",
    "        errs_arr = np.array([d[2] for d in pc1_data])\n",
    "        corr_pc1 = np.corrcoef(pc1_arr, errs_arr)[0, 1]\n",
    "        ax.set_title(f'Steerability vs |PC1 Loading| (r={corr_pc1:.2f})')\n",
    "    else:\n",
    "        ax.set_title('Steerability vs |PC1 Loading| (no data)')\n",
    "    ax.set_xlabel('Mean |PC1 Loading|')\n",
    "    ax.set_ylabel('Mean Error')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heatmap"
   },
   "outputs": [],
   "source": [
    "# Heatmap: Optimal Steering Strength per Variable × Party\n",
    "optimal_data = []\n",
    "\n",
    "for var_name in metrics_df['variable']:  # Already sorted by mean_error\n",
    "    row = {'variable': var_name}\n",
    "    llm_df = llm_steering_results[var_name]\n",
    "    var_gss = gss_probs[var_name]\n",
    "\n",
    "    for party_name in PARTYID_MAPPING:\n",
    "        target = var_gss.get(party_name, {}).get('prob_a')\n",
    "        strength, _ = find_optimal_strength(llm_df, target)\n",
    "        row[party_name] = strength\n",
    "\n",
    "    optimal_data.append(row)\n",
    "\n",
    "optimal_df = pd.DataFrame(optimal_data).set_index('variable')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, max(8, len(optimal_df) * 0.35)))\n",
    "sns.heatmap(\n",
    "    optimal_df.astype(float),\n",
    "    cmap='RdBu_r', center=0, vmin=-5, vmax=5,\n",
    "    annot=True, fmt='.1f', annot_kws={'size': 8},\n",
    "    cbar_kws={'label': 'Optimal Steering Strength'},\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title('Optimal Steering Strength to Match GSS Partisan Targets\\n'\n",
    "             '(negative = Dem direction, positive = Rep direction)\\n'\n",
    "             'Rows sorted by mean_error (top = most steerable)', fontsize=11)\n",
    "ax.set_ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**Steerability metrics:**\n",
    "- `mean_error`: average |LLM_optimal − GSS_target| across 5 parties (lower = more steerable)\n",
    "- `llm_range`: max − min P(A) across all strengths (how much the vector moves this attitude)\n",
    "- `gss_spread`: max − min P(A) across parties (how much real-world partisan variation exists)\n",
    "- `all_reachable`: whether all 5 GSS targets fall within the LLM's achievable range\n",
    "\n",
    "**Questions addressed:**\n",
    "1. Which of the 34 attitude variables are most/least steerable to real-world partisan levels?\n",
    "2. Do the 4 original Phase 3.1 variables (abortion, gun control, death penalty, same-sex marriage) reproduce their earlier results?\n",
    "3. Does steerability correlate with attitude vector magnitude, PC1 loading, or real-world partisan spread?\n",
    "4. Does the heatmap show the expected pattern (negative strengths for Dem targets, positive for Rep targets)?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}