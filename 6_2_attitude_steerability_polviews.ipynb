{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Phase 6.2: Attitude Steerability (Political Views)\n",
    "\n",
    "**Research Question:** How does the Extremely Conservative vs Extremely Liberal steering vector compare to the Strong Republican vs Strong Democrat vector (Phase 6.1) in steering LLM attitudes to match real-world GSS distributions?\n",
    "\n",
    "Phase 6.1 used `PartyID_Strong Republican - PartyID_Strong Democrat` as the contrastive steering vector and compared against GSS distributions broken down by party ID. This notebook replicates the same experiment using `PolViews_extremely conservative - PolViews_extremely liberal` as the steering vector and compares against GSS distributions broken down by political views (7 categories from Extremely Liberal to Extremely Conservative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Paths\n",
    "BASE_DIR = '/content/drive/MyDrive/belief_and_llms_v0'\n",
    "VECTOR_DIR = os.path.join(BASE_DIR, 'vectors')\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "\n",
    "# Constants\n",
    "STEERING_LAYER = 13\n",
    "STRENGTHS = np.arange(-5.0, 5.5, 0.5)\n",
    "\n",
    "# Load model\n",
    "model_name = 'meta-llama/Meta-Llama-3-8B-Instruct'\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "print('Model loaded.')\n",
    "\n",
    "# Load contrastive steering vector: Extremely Conservative - Extremely Liberal\n",
    "demo_vectors = torch.load(\n",
    "    os.path.join(VECTOR_DIR, f'gss_demographic_vectors_layer{STEERING_LAYER}.pt'),\n",
    "    map_location='cpu', weights_only=True,\n",
    ")\n",
    "v_con = demo_vectors['PolViews_person with an extremely conservative political view']['vector']\n",
    "v_lib = demo_vectors['PolViews_person with an extremely liberal political view']['vector']\n",
    "contrastive_vec = v_con - v_lib\n",
    "print(f'Contrastive vector (Ext Conservative - Ext Liberal) magnitude: {contrastive_vec.norm():.4f}')\n",
    "\n",
    "# Also load PartyID vector for comparison\n",
    "v_rep = demo_vectors['PartyID_Strong Republican']['vector']\n",
    "v_dem = demo_vectors['PartyID_Strong Democrat']['vector']\n",
    "partyid_vec = v_rep - v_dem\n",
    "cos_sim = F.cosine_similarity(contrastive_vec.unsqueeze(0), partyid_vec.unsqueeze(0)).item()\n",
    "print(f'PartyID contrastive vector magnitude: {partyid_vec.norm():.4f}')\n",
    "print(f'Cosine similarity (PolViews vs PartyID contrastive): {cos_sim:.4f}')\n",
    "print(f'Steering strengths: {STRENGTHS[0]:.1f} to {STRENGTHS[-1]:.1f} (step 0.5, {len(STRENGTHS)} values)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "attitude_configs"
   },
   "outputs": [],
   "source": [
    "# All 34 attitude variable configurations (same as Phase 6.1)\n",
    "ATTITUDE_CONFIGS = {\n",
    "    # === Binary Variables ===\n",
    "    'abortion': {\n",
    "        'gss_col': 'abany',\n",
    "        'option_a_label': 'Pro-choice',\n",
    "        'option_b_label': 'Pro-life',\n",
    "        'option_a_values': ['YES'],\n",
    "        'option_b_values': ['NO'],\n",
    "        'binary_prompt': '[INST] What is your stance on abortion? [/INST]\\n(A) Pro-choice (woman should have the right to choose)\\n(B) Pro-life (abortion should be restricted or banned)\\nAnswer:',\n",
    "    },\n",
    "    'gun_control': {\n",
    "        'gss_col': 'gunlaw',\n",
    "        'option_a_label': 'Stricter control',\n",
    "        'option_b_label': 'Gun rights',\n",
    "        'option_a_values': ['FAVOR'],\n",
    "        'option_b_values': ['OPPOSE'],\n",
    "        'binary_prompt': '[INST] What is your stance on gun control? [/INST]\\n(A) Support stricter gun control laws\\n(B) Protect Second Amendment gun rights\\nAnswer:',\n",
    "    },\n",
    "    'death_penalty': {\n",
    "        'gss_col': 'cappun',\n",
    "        'option_a_label': 'Oppose',\n",
    "        'option_b_label': 'Favor',\n",
    "        'option_a_values': ['OPPOSE'],\n",
    "        'option_b_values': ['FAVOR'],\n",
    "        'binary_prompt': '[INST] What is your stance on the death penalty for persons convicted of murder? [/INST]\\n(A) Oppose the death penalty\\n(B) Favor the death penalty\\nAnswer:',\n",
    "    },\n",
    "    'same_sex_marriage': {\n",
    "        'gss_col': 'MARSAME1',\n",
    "        'option_a_label': 'Support',\n",
    "        'option_b_label': 'Oppose',\n",
    "        'option_a_values': ['Strongly agree', 'Agree'],\n",
    "        'option_b_values': ['Disagree', 'Strongly disagree'],\n",
    "        'binary_prompt': '[INST] What is your stance on same-sex marriage? [/INST]\\n(A) Support (same-sex couples should have the right to marry)\\n(B) Oppose (marriage should be between a man and a woman)\\nAnswer:',\n",
    "    },\n",
    "    'free_speech_racists': {\n",
    "        'gss_col': 'librac',\n",
    "        'option_a_label': 'Allow',\n",
    "        'option_b_label': 'Censor',\n",
    "        'option_a_values': ['NOT REMOVE'],\n",
    "        'option_b_values': ['REMOVE'],\n",
    "        'binary_prompt': '[INST] Should a book written by a racist be removed from the public library? [/INST]\\n(A) Should not be removed (allow racist speech)\\n(B) Should be removed (censor racist speech)\\nAnswer:',\n",
    "    },\n",
    "    'free_speech_atheists': {\n",
    "        'gss_col': 'libath',\n",
    "        'option_a_label': 'Allow',\n",
    "        'option_b_label': 'Censor',\n",
    "        'option_a_values': ['NOT REMOVE'],\n",
    "        'option_b_values': ['REMOVE'],\n",
    "        'binary_prompt': '[INST] Should a book written against churches and religion be removed from the public library? [/INST]\\n(A) Should not be removed (allow atheist speech)\\n(B) Should be removed (censor atheist speech)\\nAnswer:',\n",
    "    },\n",
    "    'marijuana': {\n",
    "        'gss_col': 'grass',\n",
    "        'option_a_label': 'Legal',\n",
    "        'option_b_label': 'Illegal',\n",
    "        'option_a_values': ['Should be legal'],\n",
    "        'option_b_values': ['Should not be legal'],\n",
    "        'binary_prompt': '[INST] Do you think the use of marijuana should be made legal or not? [/INST]\\n(A) Should be made legal\\n(B) Should not be made legal\\nAnswer:',\n",
    "    },\n",
    "    'euthanasia': {\n",
    "        'gss_col': 'letdie1',\n",
    "        'option_a_label': 'Right to die',\n",
    "        'option_b_label': 'No right to die',\n",
    "        'option_a_values': ['YES'],\n",
    "        'option_b_values': ['NO'],\n",
    "        'binary_prompt': '[INST] When a person has an incurable disease, should doctors be allowed by law to end the patient\\'s life if the patient and family request it? [/INST]\\n(A) Yes, doctors should be allowed\\n(B) No, doctors should not be allowed\\nAnswer:',\n",
    "    },\n",
    "    'work_ethic_if_rich': {\n",
    "        'gss_col': 'richwork',\n",
    "        'option_a_label': 'Continue working',\n",
    "        'option_b_label': 'Stop working',\n",
    "        'option_a_values': ['Continue to work'],\n",
    "        'option_b_values': ['Stop working'],\n",
    "        'binary_prompt': '[INST] If you were to get enough money to live comfortably for the rest of your life, would you continue to work or stop working? [/INST]\\n(A) Continue to work\\n(B) Stop working\\nAnswer:',\n",
    "    },\n",
    "\n",
    "    # === 3-Option Variables (collapse to 2 extremes) ===\n",
    "    'spending_space': {\n",
    "        'gss_col': 'natspac',\n",
    "        'option_a_label': 'Too little',\n",
    "        'option_b_label': 'Too much',\n",
    "        'option_a_values': ['TOO LITTLE'],\n",
    "        'option_b_values': ['TOO MUCH'],\n",
    "        'binary_prompt': '[INST] Do you think we are spending too much, too little, or about the right amount on the space exploration program? [/INST]\\n(A) Too little\\n(B) Too much\\nAnswer:',\n",
    "    },\n",
    "    'spending_environment': {\n",
    "        'gss_col': 'natenvir',\n",
    "        'option_a_label': 'Too little',\n",
    "        'option_b_label': 'Too much',\n",
    "        'option_a_values': ['TOO LITTLE'],\n",
    "        'option_b_values': ['TOO MUCH'],\n",
    "        'binary_prompt': '[INST] Do you think we are spending too much, too little, or about the right amount on improving and protecting the environment? [/INST]\\n(A) Too little\\n(B) Too much\\nAnswer:',\n",
    "    },\n",
    "    'spending_crime': {\n",
    "        'gss_col': 'natcrime',\n",
    "        'option_a_label': 'Too little',\n",
    "        'option_b_label': 'Too much',\n",
    "        'option_a_values': ['TOO LITTLE'],\n",
    "        'option_b_values': ['TOO MUCH'],\n",
    "        'binary_prompt': '[INST] Do you think we are spending too much, too little, or about the right amount on halting the rising crime rate? [/INST]\\n(A) Too little\\n(B) Too much\\nAnswer:',\n",
    "    },\n",
    "    'spending_drugs': {\n",
    "        'gss_col': 'natdrug',\n",
    "        'option_a_label': 'Too little',\n",
    "        'option_b_label': 'Too much',\n",
    "        'option_a_values': ['TOO LITTLE'],\n",
    "        'option_b_values': ['TOO MUCH'],\n",
    "        'binary_prompt': '[INST] Do you think we are spending too much, too little, or about the right amount on dealing with drug addiction? [/INST]\\n(A) Too little\\n(B) Too much\\nAnswer:',\n",
    "    },\n",
    "    'spending_education': {\n",
    "        'gss_col': 'nateduc',\n",
    "        'option_a_label': 'Too little',\n",
    "        'option_b_label': 'Too much',\n",
    "        'option_a_values': ['TOO LITTLE'],\n",
    "        'option_b_values': ['TOO MUCH'],\n",
    "        'binary_prompt': '[INST] Do you think we are spending too much, too little, or about the right amount on improving the nation\\'s education system? [/INST]\\n(A) Too little\\n(B) Too much\\nAnswer:',\n",
    "    },\n",
    "    'spending_race': {\n",
    "        'gss_col': 'natrace',\n",
    "        'option_a_label': 'Too little',\n",
    "        'option_b_label': 'Too much',\n",
    "        'option_a_values': ['TOO LITTLE'],\n",
    "        'option_b_values': ['TOO MUCH'],\n",
    "        'binary_prompt': '[INST] Do you think we are spending too much, too little, or about the right amount on improving the conditions of Black Americans? [/INST]\\n(A) Too little\\n(B) Too much\\nAnswer:',\n",
    "    },\n",
    "    'spending_military': {\n",
    "        'gss_col': 'natarms',\n",
    "        'option_a_label': 'Too little',\n",
    "        'option_b_label': 'Too much',\n",
    "        'option_a_values': ['TOO LITTLE'],\n",
    "        'option_b_values': ['TOO MUCH'],\n",
    "        'binary_prompt': '[INST] Do you think we are spending too much, too little, or about the right amount on the military, armaments, and defense? [/INST]\\n(A) Too little\\n(B) Too much\\nAnswer:',\n",
    "    },\n",
    "    'spending_welfare': {\n",
    "        'gss_col': 'natfare',\n",
    "        'option_a_label': 'Too little',\n",
    "        'option_b_label': 'Too much',\n",
    "        'option_a_values': ['TOO LITTLE'],\n",
    "        'option_b_values': ['TOO MUCH'],\n",
    "        'binary_prompt': '[INST] Do you think we are spending too much, too little, or about the right amount on welfare? [/INST]\\n(A) Too little\\n(B) Too much\\nAnswer:',\n",
    "    },\n",
    "    'court_harshness': {\n",
    "        'gss_col': 'courts',\n",
    "        'option_a_label': 'Too harsh',\n",
    "        'option_b_label': 'Not harsh enough',\n",
    "        'option_a_values': ['Too harshly'],\n",
    "        'option_b_values': ['Not harshly enough'],\n",
    "        'binary_prompt': '[INST] In general, do you think the courts in this area deal too harshly or not harshly enough with criminals? [/INST]\\n(A) Too harshly\\n(B) Not harshly enough\\nAnswer:',\n",
    "    },\n",
    "    'bible': {\n",
    "        'gss_col': 'bible',\n",
    "        'option_a_label': 'Word of God',\n",
    "        'option_b_label': 'Book of fables',\n",
    "        'option_a_values': ['Word of god'],\n",
    "        'option_b_values': ['Ancient book'],\n",
    "        'binary_prompt': '[INST] Which of these statements comes closest to describing your feelings about the Bible? [/INST]\\n(A) The Bible is the actual word of God and is to be taken literally\\n(B) The Bible is an ancient book of fables, legends, history, and moral precepts\\nAnswer:',\n",
    "    },\n",
    "    'trust': {\n",
    "        'gss_col': 'trust',\n",
    "        'option_a_label': 'Can be trusted',\n",
    "        'option_b_label': \"Can't be careful\",\n",
    "        'option_a_values': ['Most people can be trusted'],\n",
    "        'option_b_values': [\"Can't be too careful\"],\n",
    "        'binary_prompt': '[INST] Generally speaking, would you say that most people can be trusted or that you can\\'t be too careful in dealing with people? [/INST]\\n(A) Most people can be trusted\\n(B) Can\\'t be too careful\\nAnswer:',\n",
    "    },\n",
    "    'fairness': {\n",
    "        'gss_col': 'fair',\n",
    "        'option_a_label': 'Fair',\n",
    "        'option_b_label': 'Take advantage',\n",
    "        'option_a_values': ['Would try to be fair'],\n",
    "        'option_b_values': ['Would take advantage of you'],\n",
    "        'binary_prompt': '[INST] Do you think most people would try to take advantage of you if they got a chance, or would they try to be fair? [/INST]\\n(A) Would try to be fair\\n(B) Would try to take advantage\\nAnswer:',\n",
    "    },\n",
    "    'open_housing': {\n",
    "        'gss_col': 'racopen',\n",
    "        'option_a_label': 'Anti-discrimination',\n",
    "        'option_b_label': 'Allow discrimination',\n",
    "        'option_a_values': ['Second law'],\n",
    "        'option_b_values': ['First law'],\n",
    "        'binary_prompt': '[INST] Suppose there is a community-wide vote on housing policy. Which law would you vote for? [/INST]\\n(A) A homeowner cannot refuse to sell to someone because of their race or color (anti-discrimination law)\\n(B) A homeowner can decide for themselves whom to sell their house to, even if they prefer not to sell to certain races (right to discriminate)\\nAnswer:',\n",
    "    },\n",
    "\n",
    "    # === Scale Variables (collapse extreme ends) ===\n",
    "    'income_equality': {\n",
    "        'gss_col': 'eqwlth',\n",
    "        'option_a_label': 'Govt reduce',\n",
    "        'option_b_label': 'Govt not reduce',\n",
    "        'option_a_values': ['The government should reduce income differences', '2'],\n",
    "        'option_b_values': ['6', 'The government should not concern itself with reducing income differences'],\n",
    "        'binary_prompt': '[INST] Some people think that the government should reduce income differences between the rich and poor. Others think the government should not concern itself with this. What do you think? [/INST]\\n(A) The government should reduce income differences\\n(B) The government should not concern itself with reducing income differences\\nAnswer:',\n",
    "    },\n",
    "    'help_poor': {\n",
    "        'gss_col': 'helppoor',\n",
    "        'option_a_label': 'Govt action',\n",
    "        'option_b_label': 'Self-help',\n",
    "        'option_a_values': ['GOVT ACTION', '2'],\n",
    "        'option_b_values': ['4', 'PEOPLE HELP SELVES'],\n",
    "        'binary_prompt': '[INST] Some people think that the government should do everything possible to improve the standard of living of all poor Americans. Others think it is not the government\\'s responsibility. What do you think? [/INST]\\n(A) Government should act to improve the poor\\'s standard of living\\n(B) People should help themselves\\nAnswer:',\n",
    "    },\n",
    "    'healthcare': {\n",
    "        'gss_col': 'helpsick',\n",
    "        'option_a_label': 'Govt help',\n",
    "        'option_b_label': 'Self-help',\n",
    "        'option_a_values': ['GOVT SHOULD HELP', '2'],\n",
    "        'option_b_values': ['4', 'PEOPLE HELP SELVES'],\n",
    "        'binary_prompt': '[INST] Some people think the government should help pay for medical care. Others think people should take care of themselves. What do you think? [/INST]\\n(A) Government should help pay for medical care\\n(B) People should help themselves\\nAnswer:',\n",
    "    },\n",
    "    'world_good_evil': {\n",
    "        'gss_col': 'world1',\n",
    "        'option_a_label': 'Good',\n",
    "        'option_b_label': 'Evil',\n",
    "        'option_a_values': ['WORLD IS GOOD', '2'],\n",
    "        'option_b_values': ['6', 'WORLD IS EVIL'],\n",
    "        'binary_prompt': '[INST] Would you say that the world is basically filled with goodness, or would you say that there is much evil in the world? [/INST]\\n(A) The world is basically good\\n(B) The world is basically evil\\nAnswer:',\n",
    "    },\n",
    "    'people_good_evil': {\n",
    "        'gss_col': 'world4',\n",
    "        'option_a_label': 'Good',\n",
    "        'option_b_label': 'Evil',\n",
    "        'option_a_values': ['PEOPLE ARE GOOD', '2'],\n",
    "        'option_b_values': ['6', 'PEOPLE ARE EVIL'],\n",
    "        'binary_prompt': '[INST] Would you say that human nature is basically good, or that it is fundamentally perverse and corrupt? [/INST]\\n(A) Human nature is basically good\\n(B) Human nature is fundamentally perverse and corrupt\\nAnswer:',\n",
    "    },\n",
    "    'perception_whites': {\n",
    "        'gss_col': 'workwhts',\n",
    "        'option_a_label': 'Hard-working',\n",
    "        'option_b_label': 'Lazy',\n",
    "        'option_a_values': ['1 - Hard-working', '2'],\n",
    "        'option_b_values': ['6', '7 - Lazy'],\n",
    "        'binary_prompt': '[INST] On a scale from hard-working to lazy, where would you rate white people in general? [/INST]\\n(A) Hard-working\\n(B) Lazy\\nAnswer:',\n",
    "    },\n",
    "    'perception_blacks': {\n",
    "        'gss_col': 'workblks',\n",
    "        'option_a_label': 'Hard-working',\n",
    "        'option_b_label': 'Lazy',\n",
    "        'option_a_values': ['1 - Hard-working', '2'],\n",
    "        'option_b_values': ['6', '7 - Lazy'],\n",
    "        'binary_prompt': '[INST] On a scale from hard-working to lazy, where would you rate Black people in general? [/INST]\\n(A) Hard-working\\n(B) Lazy\\nAnswer:',\n",
    "    },\n",
    "    'perception_hispanics': {\n",
    "        'gss_col': 'workhsps',\n",
    "        'option_a_label': 'Hard-working',\n",
    "        'option_b_label': 'Lazy',\n",
    "        'option_a_values': ['1 - Hard-working', '2'],\n",
    "        'option_b_values': ['6', '7 - Lazy'],\n",
    "        'binary_prompt': '[INST] On a scale from hard-working to lazy, where would you rate Hispanic people in general? [/INST]\\n(A) Hard-working\\n(B) Lazy\\nAnswer:',\n",
    "    },\n",
    "\n",
    "    # === 4-Option Variables (collapse to 2 groups) ===\n",
    "    'affirmative_action': {\n",
    "        'gss_col': 'affrmact',\n",
    "        'option_a_label': 'Favor',\n",
    "        'option_b_label': 'Oppose',\n",
    "        'option_a_values': ['Strongly favors', 'Not strongly favors'],\n",
    "        'option_b_values': ['Not strongly opposes', 'Strongly opposes'],\n",
    "        'binary_prompt': '[INST] What about preferential hiring and promotion of Black Americans? Do you favor or oppose it? [/INST]\\n(A) Favor affirmative action\\n(B) Oppose affirmative action\\nAnswer:',\n",
    "    },\n",
    "    'premarital_sex': {\n",
    "        'gss_col': 'premarsx',\n",
    "        'option_a_label': 'Not wrong',\n",
    "        'option_b_label': 'Wrong',\n",
    "        'option_a_values': ['NOT WRONG AT ALL', 'SOMETIMES WRONG'],\n",
    "        'option_b_values': ['ALMST ALWAYS WRG', 'ALWAYS WRONG'],\n",
    "        'binary_prompt': '[INST] If a man and woman have sexual relations before marriage, do you think it is wrong or not wrong? [/INST]\\n(A) Not wrong at all or sometimes wrong\\n(B) Almost always or always wrong\\nAnswer:',\n",
    "    },\n",
    "    'extramarital_sex': {\n",
    "        'gss_col': 'xmarsex',\n",
    "        'option_a_label': 'Not wrong',\n",
    "        'option_b_label': 'Wrong',\n",
    "        'option_a_values': ['NOT WRONG AT ALL', 'SOMETIMES WRONG'],\n",
    "        'option_b_values': ['ALMST ALWAYS WRG', 'ALWAYS WRONG'],\n",
    "        'binary_prompt': '[INST] What about a married person having sexual relations with someone other than the marriage partner? Is it wrong or not wrong? [/INST]\\n(A) Not wrong at all or sometimes wrong\\n(B) Almost always or always wrong\\nAnswer:',\n",
    "    },\n",
    "    'spanking': {\n",
    "        'gss_col': 'spanking',\n",
    "        'option_a_label': 'Appropriate',\n",
    "        'option_b_label': 'Not appropriate',\n",
    "        'option_a_values': ['STRONGLY AGREE', 'AGREE'],\n",
    "        'option_b_values': ['DISAGREE', 'STRONGLY DISAGREE'],\n",
    "        'binary_prompt': '[INST] Do you strongly agree, agree, disagree, or strongly disagree that it is sometimes necessary to discipline a child with a good, hard spanking? [/INST]\\n(A) Agree (spanking is sometimes necessary)\\n(B) Disagree (spanking is not appropriate)\\nAnswer:',\n",
    "    },\n",
    "}\n",
    "\n",
    "# Group assignments for visualization\n",
    "VARIABLE_GROUPS = {\n",
    "    'Social issues': ['abortion', 'gun_control', 'death_penalty', 'same_sex_marriage',\n",
    "                      'marijuana', 'euthanasia', 'premarital_sex', 'extramarital_sex',\n",
    "                      'spanking', 'free_speech_racists', 'free_speech_atheists'],\n",
    "    'Spending': ['spending_space', 'spending_environment', 'spending_crime', 'spending_drugs',\n",
    "                 'spending_education', 'spending_race', 'spending_military', 'spending_welfare'],\n",
    "    'Other attitudes': ['income_equality', 'help_poor', 'healthcare', 'court_harshness',\n",
    "                        'bible', 'trust', 'fairness', 'open_housing',\n",
    "                        'affirmative_action', 'work_ethic_if_rich',\n",
    "                        'world_good_evil', 'people_good_evil',\n",
    "                        'perception_whites', 'perception_blacks', 'perception_hispanics'],\n",
    "}\n",
    "\n",
    "print(f'Defined {len(ATTITUDE_CONFIGS)} attitude variable configs')\n",
    "for group, vars_ in VARIABLE_GROUPS.items():\n",
    "    print(f'  {group}: {len(vars_)} variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gss_probs"
   },
   "outputs": [],
   "source": [
    "# Load GSS data and compute real-world probabilities by political views\n",
    "gss_df = pd.read_excel(os.path.join(DATA_DIR, 'GSS.xlsx'))\n",
    "print(f'Loaded GSS data: {len(gss_df)} rows')\n",
    "\n",
    "# Political views mapping (string values in GSS.xlsx)\n",
    "POLVIEWS_MAPPING = {\n",
    "    'Extremely Liberal': 'Extremely liberal',\n",
    "    'Liberal': 'Liberal',\n",
    "    'Slightly Liberal': 'Slightly liberal',\n",
    "    'Moderate': 'Moderate, middle of the road',\n",
    "    'Slightly Conservative': 'Slightly conservative',\n",
    "    'Conservative': 'Conservative',\n",
    "    'Extremely Conservative': 'Extremely conservative',\n",
    "}\n",
    "\n",
    "POLVIEWS_COLORS = {\n",
    "    'Extremely Liberal': '#0015BC',\n",
    "    'Liberal': '#4466DD',\n",
    "    'Slightly Liberal': '#6699FF',\n",
    "    'Moderate': '#808080',\n",
    "    'Slightly Conservative': '#FF8888',\n",
    "    'Conservative': '#DD4444',\n",
    "    'Extremely Conservative': '#E81B23',\n",
    "}\n",
    "\n",
    "def compute_gss_probability(gss_df, config, polviews_gss_value=None):\n",
    "    \"\"\"Compute P(Option A) for a given attitude variable and optional polviews filter.\"\"\"\n",
    "    col = config['gss_col']\n",
    "    if col not in gss_df.columns:\n",
    "        return None, 0\n",
    "\n",
    "    df = gss_df if polviews_gss_value is None else gss_df[gss_df['polviews'] == polviews_gss_value]\n",
    "\n",
    "    valid_values = config['option_a_values'] + config['option_b_values']\n",
    "    valid_df = df[df[col].isin(valid_values)]\n",
    "\n",
    "    if len(valid_df) == 0:\n",
    "        return None, 0\n",
    "\n",
    "    option_a_count = valid_df[col].isin(config['option_a_values']).sum()\n",
    "    return option_a_count / len(valid_df), len(valid_df)\n",
    "\n",
    "# Compute probabilities for all variables and polviews categories\n",
    "gss_probs = {}\n",
    "header = f'{\"Variable\":<25s} {\"GSS col\":<12s}'\n",
    "for pv in POLVIEWS_MAPPING:\n",
    "    header += f' {pv[:7]:>8s}'\n",
    "header += f' {\"Overall\":>8s}'\n",
    "print(f'\\n{header}')\n",
    "print('-' * (25 + 12 + 8 * (len(POLVIEWS_MAPPING) + 1) + len(POLVIEWS_MAPPING) + 1))\n",
    "\n",
    "skipped = []\n",
    "for var_name, config in ATTITUDE_CONFIGS.items():\n",
    "    gss_probs[var_name] = {}\n",
    "\n",
    "    row = f'{var_name:<25s} {config[\"gss_col\"]:<12s}'\n",
    "    for pv_label, pv_gss_val in POLVIEWS_MAPPING.items():\n",
    "        prob, n = compute_gss_probability(gss_df, config, pv_gss_val)\n",
    "        gss_probs[var_name][pv_label] = {'prob_a': prob, 'n': n}\n",
    "        row += f' {prob:>7.1%}' if prob is not None else f' {\"N/A\":>7s}'\n",
    "\n",
    "    prob_all, n_all = compute_gss_probability(gss_df, config)\n",
    "    gss_probs[var_name]['Overall'] = {'prob_a': prob_all, 'n': n_all}\n",
    "    row += f' {prob_all:>7.1%}' if prob_all is not None else f' {\"N/A\":>7s}'\n",
    "\n",
    "    if prob_all is None:\n",
    "        skipped.append(var_name)\n",
    "    print(row)\n",
    "\n",
    "if skipped:\n",
    "    print(f'\\nSkipped (column not found): {skipped}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "steering_functions"
   },
   "outputs": [],
   "source": [
    "# Steering functions (reused from Phase 3.1)\n",
    "\n",
    "def get_binary_choice_prob(prompt, steering_vector, strength, layer_idx):\n",
    "    \"\"\"Get P(A) vs P(B) with steering applied via forward hook.\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)\n",
    "    steer_vec = steering_vector.to(model.device).to(model.dtype)\n",
    "\n",
    "    def steering_hook(module, input, output):\n",
    "        perturbation = steer_vec.view(1, 1, -1) * strength\n",
    "        if isinstance(output, tuple):\n",
    "            return (output[0] + perturbation,) + output[1:]\n",
    "        elif isinstance(output, torch.Tensor):\n",
    "            return output + perturbation\n",
    "        return output\n",
    "\n",
    "    layer = model.model.layers[layer_idx]\n",
    "    handle = layer.register_forward_hook(steering_hook)\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "    finally:\n",
    "        handle.remove()\n",
    "\n",
    "    next_token_logits = outputs.logits[0, -1, :]\n",
    "    id_a = tokenizer.encode(' A', add_special_tokens=False)[-1]\n",
    "    id_b = tokenizer.encode(' B', add_special_tokens=False)[-1]\n",
    "    probs = F.softmax(torch.tensor([next_token_logits[id_a], next_token_logits[id_b]]), dim=0)\n",
    "    return probs[0].item()\n",
    "\n",
    "\n",
    "def find_optimal_strength(llm_results_df, target_prob):\n",
    "    \"\"\"Find steering strength that produces P(A) closest to target.\"\"\"\n",
    "    if target_prob is None:\n",
    "        return None, None\n",
    "    errors = np.abs(llm_results_df['prob_a'].values - target_prob)\n",
    "    best_idx = np.argmin(errors)\n",
    "    return llm_results_df.iloc[best_idx]['strength'], errors[best_idx]\n",
    "\n",
    "\n",
    "print('Steering functions defined.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "steering_sweep"
   },
   "outputs": [],
   "source": [
    "# Run steering sweep for all 34 variables\n",
    "\n",
    "active_vars = [v for v in ATTITUDE_CONFIGS if gss_probs[v]['Overall']['prob_a'] is not None]\n",
    "print(f'Running steering sweep for {len(active_vars)} variables with valid GSS data')\n",
    "print(f'Strengths: {STRENGTHS[0]:.1f} to {STRENGTHS[-1]:.1f} ({len(STRENGTHS)} values)')\n",
    "print(f'Total forward passes: {len(active_vars) * len(STRENGTHS)}')\n",
    "\n",
    "llm_steering_results = {}\n",
    "\n",
    "for i, var_name in enumerate(active_vars):\n",
    "    config = ATTITUDE_CONFIGS[var_name]\n",
    "    prompt = config['binary_prompt']\n",
    "\n",
    "    results = []\n",
    "    for strength in STRENGTHS:\n",
    "        prob_a = get_binary_choice_prob(prompt, contrastive_vec, strength, STEERING_LAYER)\n",
    "        results.append({'strength': strength, 'prob_a': prob_a})\n",
    "\n",
    "    llm_steering_results[var_name] = pd.DataFrame(results)\n",
    "    print(f'  [{i+1}/{len(active_vars)}] {var_name}: '\n",
    "          f'P(A) range = {llm_steering_results[var_name][\"prob_a\"].min():.3f} \\u2013 '\n",
    "          f'{llm_steering_results[var_name][\"prob_a\"].max():.3f}')\n",
    "\n",
    "# Save results\n",
    "save_path = os.path.join(DATA_DIR, 'attitude_steerability_polviews_results.pt')\n",
    "torch.save({\n",
    "    'llm_steering_results': {k: v.to_dict() for k, v in llm_steering_results.items()},\n",
    "    'gss_probs': gss_probs,\n",
    "    'strengths': STRENGTHS.tolist(),\n",
    "    'steering_layer': STEERING_LAYER,\n",
    "    'steering_vector': 'PolViews_ExtConservative_minus_ExtLiberal',\n",
    "}, save_path)\n",
    "print(f'\\nResults saved to {save_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz_page1"
   },
   "outputs": [],
   "source": [
    "# Visualize: LLM Steering Curves + GSS Target Lines (Page 1: Variables 1-12)\n",
    "active_vars = list(llm_steering_results.keys())\n",
    "\n",
    "def plot_steering_page(var_names, page_num, total_pages):\n",
    "    n = len(var_names)\n",
    "    ncols = 4\n",
    "    nrows = (n + ncols - 1) // ncols\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(20, 4 * nrows))\n",
    "    axes = axes.flatten() if nrows > 1 else (axes if n > 1 else [axes])\n",
    "\n",
    "    for idx, var_name in enumerate(var_names):\n",
    "        ax = axes[idx]\n",
    "        config = ATTITUDE_CONFIGS[var_name]\n",
    "        llm_df = llm_steering_results[var_name]\n",
    "        var_gss = gss_probs[var_name]\n",
    "\n",
    "        # LLM steering curve\n",
    "        ax.plot(llm_df['strength'], llm_df['prob_a'], 'k-o', linewidth=2.5, markersize=3,\n",
    "                label='LLM P(A)', zorder=10)\n",
    "\n",
    "        # Overall GSS line\n",
    "        overall = var_gss.get('Overall', {})\n",
    "        if overall.get('prob_a') is not None:\n",
    "            ax.axhline(y=overall['prob_a'], color='purple', linestyle='-', linewidth=3, alpha=0.8,\n",
    "                       label=f'All GSS: {overall[\"prob_a\"]:.1%}')\n",
    "\n",
    "        # Per-polviews GSS lines\n",
    "        for pv_name in POLVIEWS_MAPPING:\n",
    "            pv_data = var_gss.get(pv_name, {})\n",
    "            if pv_data.get('prob_a') is not None:\n",
    "                ax.axhline(y=pv_data['prob_a'], color=POLVIEWS_COLORS[pv_name],\n",
    "                           linestyle='--', linewidth=1.5,\n",
    "                           label=f'{pv_name}: {pv_data[\"prob_a\"]:.1%}')\n",
    "\n",
    "        ax.axhline(y=0.5, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "        ax.axvline(x=0, color='gray', linestyle=':', linewidth=1, alpha=0.5)\n",
    "        ax.set_xlabel('Steering Strength\\n(\\u2190Liberal | Conservative\\u2192)')\n",
    "        ax.set_ylabel(f'P({config[\"option_a_label\"]})')\n",
    "        ax.set_title(f'{var_name.replace(\"_\", \" \").title()}\\n({config[\"gss_col\"]})', fontsize=10)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.legend(loc='best', fontsize=5)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "\n",
    "    for idx in range(n, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "\n",
    "    plt.suptitle(f'LLM Steering (PolViews) vs GSS Probabilities \\u2014 Page {page_num}/{total_pages}', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Page 1\n",
    "plot_steering_page(active_vars[:12], 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz_page2"
   },
   "outputs": [],
   "source": [
    "# Page 2: Variables 13-24\n",
    "plot_steering_page(active_vars[12:24], 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz_page3"
   },
   "outputs": [],
   "source": [
    "# Page 3: Variables 25-34\n",
    "plot_steering_page(active_vars[24:], 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "steerability_metrics"
   },
   "outputs": [],
   "source": [
    "# Compute steerability metrics\n",
    "metrics = []\n",
    "\n",
    "for var_name in active_vars:\n",
    "    config = ATTITUDE_CONFIGS[var_name]\n",
    "    llm_df = llm_steering_results[var_name]\n",
    "    var_gss = gss_probs[var_name]\n",
    "\n",
    "    llm_min = llm_df['prob_a'].min()\n",
    "    llm_max = llm_df['prob_a'].max()\n",
    "    llm_range = llm_max - llm_min\n",
    "\n",
    "    errors = []\n",
    "    all_reachable = True\n",
    "    gss_pv_probs = []\n",
    "\n",
    "    for pv_name in POLVIEWS_MAPPING:\n",
    "        pv_data = var_gss.get(pv_name, {})\n",
    "        target = pv_data.get('prob_a')\n",
    "        if target is None:\n",
    "            continue\n",
    "\n",
    "        gss_pv_probs.append(target)\n",
    "        _, error = find_optimal_strength(llm_df, target)\n",
    "        errors.append(error)\n",
    "\n",
    "        if target < llm_min or target > llm_max:\n",
    "            all_reachable = False\n",
    "\n",
    "    gss_spread = max(gss_pv_probs) - min(gss_pv_probs) if gss_pv_probs else 0\n",
    "    mean_error = np.mean(errors) if errors else None\n",
    "    max_error = np.max(errors) if errors else None\n",
    "\n",
    "    group = 'Unknown'\n",
    "    for g, vars_ in VARIABLE_GROUPS.items():\n",
    "        if var_name in vars_:\n",
    "            group = g\n",
    "            break\n",
    "\n",
    "    metrics.append({\n",
    "        'variable': var_name,\n",
    "        'gss_col': config['gss_col'],\n",
    "        'group': group,\n",
    "        'llm_range': llm_range,\n",
    "        'mean_error': mean_error,\n",
    "        'max_error': max_error,\n",
    "        'all_reachable': all_reachable,\n",
    "        'gss_spread': gss_spread,\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics).sort_values('mean_error')\n",
    "\n",
    "print('=== STEERABILITY METRICS \\u2014 PolViews (sorted by mean_error, lower = more steerable) ===')\n",
    "print(f'\\n{\"Variable\":<25s} {\"Group\":<18s} {\"LLM Range\":>10s} {\"Mean Err\":>9s} {\"Max Err\":>9s} {\"GSS Spread\":>10s} {\"Reachable\":>9s}')\n",
    "print('-' * 95)\n",
    "for _, row in metrics_df.iterrows():\n",
    "    print(f'{row[\"variable\"]:<25s} {row[\"group\"]:<18s} '\n",
    "          f'{row[\"llm_range\"]:>10.3f} {row[\"mean_error\"]:>9.3f} {row[\"max_error\"]:>9.3f} '\n",
    "          f'{row[\"gss_spread\"]:>10.3f} {\"\\u2713\" if row[\"all_reachable\"] else \"\\u2717\":>9s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "steerability_bar"
   },
   "outputs": [],
   "source": [
    "# Steerability summary bar chart\n",
    "GROUP_COLORS = {\n",
    "    'Social issues': '#E74C3C',\n",
    "    'Spending': '#3498DB',\n",
    "    'Other attitudes': '#2ECC71',\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "bars = ax.barh(\n",
    "    range(len(metrics_df)),\n",
    "    metrics_df['mean_error'],\n",
    "    color=[GROUP_COLORS.get(row['group'], '#888') for _, row in metrics_df.iterrows()],\n",
    ")\n",
    "\n",
    "ax.set_yticks(range(len(metrics_df)))\n",
    "ax.set_yticklabels([f'{row[\"variable\"]}' for _, row in metrics_df.iterrows()], fontsize=9)\n",
    "ax.set_xlabel('Mean Error (|LLM optimal - GSS target|)')\n",
    "ax.set_title('Attitude Steerability (PolViews vector): Mean Error by Variable\\n(lower = more steerable)')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=c, label=g) for g, c in GROUP_COLORS.items()]\n",
    "ax.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "ax.grid(True, axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "comparison_6_1"
   },
   "outputs": [],
   "source": [
    "# Compare PolViews steerability with PartyID steerability (Phase 6.1)\n",
    "\n",
    "# Load Phase 6.1 results\n",
    "partyid_results_path = os.path.join(DATA_DIR, 'attitude_steerability_results.pt')\n",
    "partyid_saved = torch.load(partyid_results_path, map_location='cpu', weights_only=False)\n",
    "\n",
    "# Reconstruct Phase 6.1 metrics\n",
    "partyid_llm_results = {k: pd.DataFrame(v) for k, v in partyid_saved['llm_steering_results'].items()}\n",
    "partyid_gss_probs = partyid_saved['gss_probs']\n",
    "\n",
    "PARTYID_MAPPING = {\n",
    "    'Strong Democrat': 'Strong democrat',\n",
    "    'Democrat': 'Not very strong democrat',\n",
    "    'Independent': 'Independent (neither, no response)',\n",
    "    'Republican': 'Not very strong republican',\n",
    "    'Strong Republican': 'Strong republican',\n",
    "}\n",
    "\n",
    "partyid_metrics = []\n",
    "for var_name in partyid_llm_results:\n",
    "    llm_df = partyid_llm_results[var_name]\n",
    "    var_gss = partyid_gss_probs[var_name]\n",
    "\n",
    "    errors = []\n",
    "    for party_name in PARTYID_MAPPING:\n",
    "        target = var_gss.get(party_name, {}).get('prob_a')\n",
    "        if target is None:\n",
    "            continue\n",
    "        _, error = find_optimal_strength(llm_df, target)\n",
    "        errors.append(error)\n",
    "\n",
    "    partyid_metrics.append({\n",
    "        'variable': var_name,\n",
    "        'partyid_mean_error': np.mean(errors) if errors else None,\n",
    "    })\n",
    "\n",
    "partyid_metrics_df = pd.DataFrame(partyid_metrics)\n",
    "\n",
    "# Merge with polviews metrics\n",
    "comparison_df = metrics_df[['variable', 'group', 'mean_error']].rename(\n",
    "    columns={'mean_error': 'polviews_mean_error'}\n",
    ").merge(partyid_metrics_df, on='variable')\n",
    "\n",
    "# Scatter plot: PartyID vs PolViews steerability\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Panel 1: Scatter comparison\n",
    "ax = axes[0]\n",
    "for _, row in comparison_df.iterrows():\n",
    "    color = GROUP_COLORS.get(row['group'], '#888')\n",
    "    ax.scatter(row['partyid_mean_error'], row['polviews_mean_error'], c=color, s=80, zorder=5)\n",
    "    ax.annotate(row['variable'], (row['partyid_mean_error'], row['polviews_mean_error']),\n",
    "                fontsize=6, ha='left', va='bottom')\n",
    "\n",
    "# Diagonal line\n",
    "lim = max(comparison_df['partyid_mean_error'].max(), comparison_df['polviews_mean_error'].max()) * 1.1\n",
    "ax.plot([0, lim], [0, lim], 'k--', alpha=0.3, label='Equal steerability')\n",
    "\n",
    "corr = comparison_df[['partyid_mean_error', 'polviews_mean_error']].corr().iloc[0, 1]\n",
    "ax.set_xlabel('PartyID Mean Error (Phase 6.1)')\n",
    "ax.set_ylabel('PolViews Mean Error (Phase 6.2)')\n",
    "ax.set_title(f'PartyID vs PolViews Steerability (r={corr:.2f})\\nBelow diagonal = PolViews better')\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Panel 2: Paired bar chart (top 15 by average error)\n",
    "ax = axes[1]\n",
    "comparison_df['avg_error'] = (comparison_df['partyid_mean_error'] + comparison_df['polviews_mean_error']) / 2\n",
    "top_df = comparison_df.sort_values('avg_error').head(15)\n",
    "\n",
    "y = np.arange(len(top_df))\n",
    "height = 0.35\n",
    "ax.barh(y - height/2, top_df['partyid_mean_error'], height, label='PartyID (6.1)', color='#E74C3C', alpha=0.7)\n",
    "ax.barh(y + height/2, top_df['polviews_mean_error'], height, label='PolViews (6.2)', color='#3498DB', alpha=0.7)\n",
    "ax.set_yticks(y)\n",
    "ax.set_yticklabels(top_df['variable'], fontsize=9)\n",
    "ax.set_xlabel('Mean Error')\n",
    "ax.set_title('Top 15 Most Steerable Variables:\\nPartyID vs PolViews')\n",
    "ax.invert_yaxis()\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison table\n",
    "print(f'\\n{\"Variable\":<25s} {\"PartyID Err\":>12s} {\"PolViews Err\":>13s} {\"Diff\":>8s} {\"Better\":>10s}')\n",
    "print('-' * 72)\n",
    "for _, row in comparison_df.sort_values('avg_error').iterrows():\n",
    "    diff = row['polviews_mean_error'] - row['partyid_mean_error']\n",
    "    better = 'PolViews' if diff < 0 else 'PartyID'\n",
    "    print(f'{row[\"variable\"]:<25s} {row[\"partyid_mean_error\"]:>12.3f} {row[\"polviews_mean_error\"]:>13.3f} '\n",
    "          f'{diff:>+8.3f} {better:>10s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "heatmap"
   },
   "outputs": [],
   "source": [
    "# Heatmap: Optimal Steering Strength per Variable x PolViews\n",
    "optimal_data = []\n",
    "\n",
    "for var_name in metrics_df['variable']:\n",
    "    row = {'variable': var_name}\n",
    "    llm_df = llm_steering_results[var_name]\n",
    "    var_gss = gss_probs[var_name]\n",
    "\n",
    "    for pv_name in POLVIEWS_MAPPING:\n",
    "        target = var_gss.get(pv_name, {}).get('prob_a')\n",
    "        strength, _ = find_optimal_strength(llm_df, target)\n",
    "        row[pv_name] = strength\n",
    "\n",
    "    optimal_data.append(row)\n",
    "\n",
    "optimal_df = pd.DataFrame(optimal_data).set_index('variable')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, max(8, len(optimal_df) * 0.35)))\n",
    "sns.heatmap(\n",
    "    optimal_df.astype(float),\n",
    "    cmap='RdBu_r', center=0, vmin=-5, vmax=5,\n",
    "    annot=True, fmt='.1f', annot_kws={'size': 7},\n",
    "    cbar_kws={'label': 'Optimal Steering Strength'},\n",
    "    ax=ax,\n",
    ")\n",
    "ax.set_title('Optimal Steering Strength to Match GSS PolViews Targets\\n'\n",
    "             '(negative = Liberal direction, positive = Conservative direction)\\n'\n",
    "             'Rows sorted by mean_error (top = most steerable)', fontsize=11)\n",
    "ax.set_ylabel('')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": [
    "## Summary\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**PolViews vs PartyID steering vectors:**\n",
    "- Cosine similarity between the two contrastive vectors tells us how aligned party and ideology are in the model's latent space\n",
    "- The comparison reveals which attitudes are better captured by ideological vs partisan identity\n",
    "\n",
    "**Steerability comparison:**\n",
    "- Variables where PolViews outperforms PartyID suggest the model encodes these attitudes more along an ideological axis than a partisan one\n",
    "- Variables where PartyID outperforms PolViews suggest partisan identity captures something beyond ideology (e.g., group loyalty, tribal signals)\n",
    "- Variables where both fail equally are likely locked by RLHF/safety training regardless of steering direction\n",
    "\n",
    "**Note:** The PolViews vector has 7 GSS target lines (vs 5 for PartyID), providing a more fine-grained comparison but also making it harder to achieve low mean error (more targets to match)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}