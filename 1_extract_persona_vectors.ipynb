{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a7d1584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54bf7cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load extraction_datasets.json\n",
    "import json\n",
    "with open(\"data/gss_extraction_datasets.json\", \"r\") as f:\n",
    "    extraction_datasets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "824b4112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Race_person of different race than Black or White', 'Race_White person', 'Race_Black person', 'PartyID_Independent leaning Republican', 'PartyID_Strong Democrat', 'PartyID_Democrat', 'PartyID_Political Independent', 'PartyID_Strong Republican', 'PartyID_Independent leaning Democrat', 'PartyID_Other', 'PartyID_Republican', 'Sex_person', 'Degree_high school graduate', 'Degree_person with less than high school education', \"Degree_person with a bachelor's degree\", 'Degree_person with some college education', 'Degree_person with a graduate degree', 'Religion_Catholic', 'Religion_Protestant', 'Religion_person of other religion', 'Religion_Jewish', 'PolViews_person with a slightly liberal political view', 'PolViews_person with a conservative political view', 'PolViews_person with a liberal political view', 'PolViews_person with a neutral political view', 'PolViews_person with a slightly conservative political view', 'PolViews_person with an extremely liberal political view', 'PolViews_person with an extremely conservative political view', 'Generation_person from Generation X', 'Generation_person from the Silent Generation', 'Generation_Baby Boomer', 'Generation_Millennial', 'Generation_person from Generation Z'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extraction_datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10a62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Setup\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Load model in 4-bit to save memory\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Choose the layer to hack\n",
    "TARGET_LAYER = 15\n",
    "SAVE_DIR = \"vectors\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# Dictionary to store the final vectors in RAM\n",
    "# Structure: {'Race_Black person': Tensor(shape=[4096]), ...}\n",
    "demographic_vectors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4860d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: Hook Function\n",
    "def get_layer_activations(model, tokenizer, inputs_text, layer_idx):\n",
    "    \"\"\"\n",
    "    Feeds a list of texts to the model and captures the hidden state of the LAST token at the specified layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize (with padding for batch processing if needed, here we do 1 by 1 for clarity)\n",
    "    inputs = tokenizer(inputs_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Placeholder\n",
    "    captured_hidden = None\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        nonlocal captured_hidden\n",
    "        # output[0] shape: [batch_size, seq_len, hidden_dim]\n",
    "        # Grab the last token (-1) for the batch\n",
    "        captured_hidden = output[0][:, -1, :].detach().cpu()\n",
    "\n",
    "    # Register hook\n",
    "    layer = model.model.layers[layer_idx]\n",
    "    handle = layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "\n",
    "\n",
    "    # Cleanup\n",
    "    handle.remove()\n",
    "    return captured_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89ca900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction Loop (main)\n",
    "print(f\"Starting extraction on Layer {TARGET_LAYER}...\")\n",
    "\n",
    "# Iterate through every group created\n",
    "for label, pairs in extraction_datasets.items():\n",
    "    print(f\"Processing: {label}...\")\n",
    "\n",
    "    # Unzip the pairs into two lists: Positives and Negatives\n",
    "    # pairs is [(pos1, neg1), (pos2, neg2), ...]\n",
    "    pos_texts = [p[0] for p in pairs]\n",
    "    neg_texts = [p[1] for p in pairs]\n",
    "\n",
    "    # 1. Get activations for X+\n",
    "    pos_acts = []\n",
    "    for text in pos_texts:\n",
    "        act = get_layer_activations(model, tokenizer, text, TARGET_LAYER)\n",
    "        pos_acts.append(act)\n",
    "    \n",
    "    # 2. Get activations for X-\n",
    "    neg_acts = []\n",
    "    for text in neg_texts:\n",
    "        act = get_layer_activations(model, tokenizer, text, TARGET_LAYER)\n",
    "        neg_acts.append(act)\n",
    "\n",
    "    # 3. Stack and compute mean (shape becomes [num_prompts, hidden_dim])\n",
    "    pos_tensor = torch.vstack(pos_acts)\n",
    "    neg_tensor = torch.vstack(neg_acts)\n",
    "\n",
    "    # 4. Calculate the difference vector (mean(Pos) - mean(Neg))\n",
    "    diff_vector = torch.mean(pos_tensor, dim=0) - torch.mean(neg_tensor, dim=0)\n",
    "\n",
    "    # 5. Normalize\n",
    "    raw_magnitude = torch.norm(diff_vector).item()\n",
    "    normalized_vector = diff_vector / torch.norm(diff_vector)\n",
    "\n",
    "    # 6. Store result\n",
    "    demographic_vectors[label] = {\n",
    "        \"vector\": normalized_vector,\n",
    "        \"magnitude\": raw_magnitude\n",
    "    }\n",
    "\n",
    "    print(f\" -> Done. Magnitude: {raw_magnitude:.4f}\")\n",
    "\n",
    "# Save results\n",
    "save_path = os.path.join(SAVE_DIR, \"gss_demogaphic_vectors.pt\")\n",
    "torch.save(demographic_vectors, save_path)\n",
    "print(f\"\\nAll vectors saved to {save_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d3d3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8059e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "def get_activations(text, model, tokenizer, layer_idx):\n",
    "    \"\"\"\n",
    "    Runs text through model and captures the hidden state at the LAST token of the specified layer.\n",
    "    \"\"\"\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # Placeholder for the activation\n",
    "    activation = {}\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        # output[0] is the hidden state tensor: (batch, seq_len, hidden_dim)\n",
    "        # Grab the last token's activation: [0, -1, :]\n",
    "        activation['hidden'] = output[0][0, -1, :].detach().cpu()\n",
    "\n",
    "    # Register the hook to the specific layer\n",
    "    layer = model.model.layers[layer_idx]\n",
    "    handle = layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    # Run the model (forward pass)\n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "\n",
    "    # Remove hook so it doesn't mess up future runs\n",
    "    handle.remove()\n",
    "\n",
    "    return activation['hidden']\n",
    "\n",
    "\n",
    "def extract_steering_vector(positive_prompts, negative_prompts, model, tokenizer, layer_idx):\n",
    "    \"\"\"\n",
    "    Calculates the direction: Mean(Pos)-Mean(Neg)\n",
    "    (PCA will be applied for large data, but here I just compute the mean difference)\n",
    "    \"\"\"\n",
    "\n",
    "    pos_acts = [get_activations(p, model, tokenizer, layer_idx) for p in positive_prompts]\n",
    "    neg_acts = [get_activations(n, model, tokenizer, layer_idx) for n in negative_prompts]\n",
    "\n",
    "    # Stack into tensors\n",
    "    pos_tensor = torch.stack(pos_acts)\n",
    "    neg_tensor = torch.stack(neg_acts)\n",
    "\n",
    "    # Simple difference of means\n",
    "    direction = torch.mean(pos_tensor, dim=0) - torch.mean(neg_tensor, dim=0)\n",
    "\n",
    "    # Normalize the vector (Unit length) to scale it manually later\n",
    "    direction = direction / torch.norm(direction)\n",
    "\n",
    "    return direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00717a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct datasets for each target\n",
    "\n",
    "# Political party\n",
    "republican_prompts = [\n",
    "    \"[INST] You are a Republican\"\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4919d734",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379480e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9765b7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "belief_llms_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
