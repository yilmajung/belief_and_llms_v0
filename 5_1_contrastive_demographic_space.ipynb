{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/yilmajung/belief_and_llms_v0/blob/main/5_1_contrastive_demographic_space.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5.1: Contrastive Demographic Space\n",
    "\n",
    "Phase 5 PCA on 120 \"X vs. generic\" demographic vectors was dominated by a shared \"demographic-ness\"\n",
    "direction (mean cosine sim ~0.39), producing a PC1 that captures attitudinal specificity rather than\n",
    "political content, and near-zero GSS validation (r~0.04).\n",
    "\n",
    "This notebook applies two approaches to strip the shared baseline and reveal cleaner demographic axes:\n",
    "\n",
    "**Approach 2 — Within-Category Centering:**\n",
    "For each of the 25 categories, subtract the category mean vector. This removes the shared\n",
    "\"demographic-ness\" direction within each category, leaving only within-category contrasts.\n",
    "\n",
    "**Approach 3 — Explicit Contrastive Pairs:**\n",
    "Compute all C(n,2) pairwise differences within each category: `v_A - v_B`. This directly\n",
    "produces oppositional vectors that encode \"A vs B\" rather than \"A vs generic.\"\n",
    "\n",
    "**No GPU required** — PCA on small matrices is CPU-only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from matplotlib.cm import get_cmap\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (Colab) or use local path\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    BASE_DIR = \"/content/drive/MyDrive/belief_and_llms_v0\"\n",
    "except ImportError:\n",
    "    BASE_DIR = \".\"\n",
    "\n",
    "VECTOR_DIR = os.path.join(BASE_DIR, \"vectors\")\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "\n",
    "# Reference layer: 13 (peak delta magnitude from Phase 3 analysis)\n",
    "REF_LAYER = 13\n",
    "\n",
    "print(f\"Base directory: {BASE_DIR}\")\n",
    "print(f\"Vector directory: {VECTOR_DIR}\")\n",
    "print(f\"Reference layer: {REF_LAYER}\")\n",
    "\n",
    "# Load vectors for the reference layer\n",
    "vectors = torch.load(\n",
    "    os.path.join(VECTOR_DIR, f\"gss_demographic_vectors_layer{REF_LAYER}.pt\"),\n",
    "    map_location=\"cpu\",\n",
    "    weights_only=True,\n",
    ")\n",
    "\n",
    "# Build sorted labels, categories, and unnormalized matrix\n",
    "labels = sorted(vectors.keys())\n",
    "categories = [label.split('_')[0] for label in labels]\n",
    "unique_cats = sorted(set(categories))\n",
    "\n",
    "# Reconstruct raw (unnormalized) vectors: direction * magnitude\n",
    "X = np.array([\n",
    "    (vectors[lab]['vector'] * vectors[lab]['magnitude']).numpy()\n",
    "    for lab in labels\n",
    "])\n",
    "magnitudes = np.array([vectors[lab]['magnitude'] for lab in labels])\n",
    "\n",
    "print(f\"\\nLoaded {len(vectors)} demographic vectors from layer {REF_LAYER}\")\n",
    "print(f\"Vector dimension: {X.shape[1]}\")\n",
    "print(f\"Matrix shape: {X.shape}\")\n",
    "print(f\"Categories ({len(unique_cats)}): {unique_cats}\")\n",
    "\n",
    "# Color palette for 25 categories (tab20 + tab20b)\n",
    "n_cats = len(unique_cats)\n",
    "tab20 = [get_cmap('tab20')(i / 20) for i in range(20)]\n",
    "tab20b = [get_cmap('tab20b')(i / 20) for i in range(20)]\n",
    "all_colors = tab20 + tab20b[:n_cats - 20] if n_cats > 20 else tab20[:n_cats]\n",
    "cat_to_color = {cat: all_colors[i] for i, cat in enumerate(unique_cats)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Within-Category Centering\n",
    "\n",
    "For each of the 25 categories, compute the mean vector of its members and subtract it.\n",
    "This removes the shared \"X vs generic\" direction within each category, isolating\n",
    "within-category contrasts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within-category centering\n",
    "X_centered = np.copy(X)\n",
    "cat_means = {}\n",
    "\n",
    "for cat in unique_cats:\n",
    "    mask = [c == cat for c in categories]\n",
    "    indices = [i for i, m in enumerate(mask) if m]\n",
    "    cat_mean = X[indices].mean(axis=0)\n",
    "    cat_means[cat] = cat_mean\n",
    "    X_centered[indices] -= cat_mean\n",
    "\n",
    "# Verify centering: per-category means should be zero vectors\n",
    "print(\"Verification: per-category mean norms after centering\")\n",
    "print(\"=\" * 50)\n",
    "for cat in unique_cats:\n",
    "    mask = [c == cat for c in categories]\n",
    "    indices = [i for i, m in enumerate(mask) if m]\n",
    "    mean_norm = np.linalg.norm(X_centered[indices].mean(axis=0))\n",
    "    n_members = len(indices)\n",
    "    print(f\"  {cat:15s}: mean_norm = {mean_norm:.2e}, n_members = {n_members}\")\n",
    "\n",
    "print(f\"\\nCentered matrix shape: {X_centered.shape}\")\n",
    "print(f\"Original matrix norm: {np.linalg.norm(X, 'fro'):.2f}\")\n",
    "print(f\"Centered matrix norm: {np.linalg.norm(X_centered, 'fro'):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on centered matrix\n",
    "n_components = min(30, len(labels))\n",
    "\n",
    "# Original PCA (for comparison)\n",
    "pca_orig = PCA(n_components=n_components)\n",
    "X_pca_orig = pca_orig.fit_transform(X)\n",
    "\n",
    "# Centered PCA\n",
    "pca_centered = PCA(n_components=n_components)\n",
    "X_pca_centered = pca_centered.fit_transform(X_centered)\n",
    "\n",
    "# Scree plot comparison: original vs centered\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Scree plot (centered)\n",
    "axes[0].bar(range(1, 21), pca_centered.explained_variance_ratio_[:20], color='steelblue', alpha=0.8)\n",
    "axes[0].set_xlabel('Principal Component', fontsize=12)\n",
    "axes[0].set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "axes[0].set_title('Scree Plot (centered vectors)', fontsize=13)\n",
    "axes[0].set_xticks(range(1, 21))\n",
    "\n",
    "# Cumulative (centered)\n",
    "cum_var_c = np.cumsum(pca_centered.explained_variance_ratio_[:20])\n",
    "axes[1].plot(range(1, 21), cum_var_c, 'bo-', linewidth=2, markersize=6)\n",
    "axes[1].axhline(0.8, color='red', linestyle='--', alpha=0.5, label='80%')\n",
    "axes[1].axhline(0.9, color='orange', linestyle='--', alpha=0.5, label='90%')\n",
    "axes[1].set_xlabel('Number of Components', fontsize=12)\n",
    "axes[1].set_ylabel('Cumulative Explained Variance', fontsize=12)\n",
    "axes[1].set_title('Cumulative Variance (centered)', fontsize=13)\n",
    "axes[1].legend()\n",
    "axes[1].set_xticks(range(1, 21))\n",
    "\n",
    "# Comparison: original vs centered\n",
    "axes[2].plot(range(1, 21), pca_orig.explained_variance_ratio_[:20], 'rs-', label='Original', linewidth=2)\n",
    "axes[2].plot(range(1, 21), pca_centered.explained_variance_ratio_[:20], 'bo-', label='Centered', linewidth=2)\n",
    "axes[2].set_xlabel('Principal Component', fontsize=12)\n",
    "axes[2].set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "axes[2].set_title('Original vs Centered', fontsize=13)\n",
    "axes[2].legend()\n",
    "axes[2].set_xticks(range(1, 21))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop 10 PCs explained variance comparison:\")\n",
    "print(f\"{'PC':>4s}  {'Original':>10s}  {'Centered':>10s}  {'Diff':>10s}\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(10):\n",
    "    orig_v = pca_orig.explained_variance_ratio_[i]\n",
    "    cent_v = pca_centered.explained_variance_ratio_[i]\n",
    "    print(f\"PC{i+1:>2d}  {orig_v:10.4f}  {cent_v:10.4f}  {cent_v - orig_v:+10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centered PCA: PC1 vs PC2 scatter\n",
    "df_centered = pd.DataFrame({\n",
    "    'PC1': X_pca_centered[:, 0],\n",
    "    'PC2': X_pca_centered[:, 1],\n",
    "    'PC3': X_pca_centered[:, 2],\n",
    "    'label': labels,\n",
    "    'category': categories,\n",
    "    'short_label': [label.split('_', 1)[1][:30] for label in labels],\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 14))\n",
    "\n",
    "for cat in unique_cats:\n",
    "    mask = df_centered['category'] == cat\n",
    "    ax.scatter(\n",
    "        df_centered.loc[mask, 'PC1'], df_centered.loc[mask, 'PC2'],\n",
    "        label=cat, color=cat_to_color[cat], s=80, alpha=0.8,\n",
    "        edgecolors='white', linewidth=0.5,\n",
    "    )\n",
    "\n",
    "for _, row in df_centered.iterrows():\n",
    "    ax.annotate(\n",
    "        row['short_label'], (row['PC1'], row['PC2']),\n",
    "        fontsize=5.5, alpha=0.8,\n",
    "        xytext=(3, 3), textcoords='offset points',\n",
    "    )\n",
    "\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.2)\n",
    "ax.axvline(0, color='black', linestyle='--', alpha=0.2)\n",
    "ax.set_xlabel(f'PC1 ({pca_centered.explained_variance_ratio_[0]:.1%} variance)', fontsize=14)\n",
    "ax.set_ylabel(f'PC2 ({pca_centered.explained_variance_ratio_[1]:.1%} variance)', fontsize=14)\n",
    "ax.set_title(f'Centered Demographic Space: PC1 vs PC2 (Layer {REF_LAYER})', fontsize=16)\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8, ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centered PCA: PC Poles (top/bottom 10 on PC1-PC5)\n",
    "print(\"CENTERED PCA — PC Poles\")\n",
    "for pc_idx in range(5):\n",
    "    pc_name = f'PC{pc_idx + 1}'\n",
    "    projections = X_pca_centered[:, pc_idx]\n",
    "    sorted_idx = np.argsort(projections)\n",
    "\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(f\"{pc_name} — Explained Variance: {pca_centered.explained_variance_ratio_[pc_idx]:.2%}\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "\n",
    "    print(f\"\\n  POSITIVE pole (top 10):\")\n",
    "    for i in sorted_idx[-10:][::-1]:\n",
    "        print(f\"    {projections[i]:+8.4f}  [{categories[i]:12s}]  {labels[i]}\")\n",
    "\n",
    "    print(f\"\\n  NEGATIVE pole (bottom 10):\")\n",
    "    for i in sorted_idx[:10]:\n",
    "        print(f\"    {projections[i]:+8.4f}  [{categories[i]:12s}]  {labels[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3: Explicit Contrastive Pairs\n",
    "\n",
    "For each category, compute all C(n,2) pairwise differences: `v_A - v_B`.\n",
    "These contrastive vectors directly encode \"A vs B\" within each category,\n",
    "completely removing any shared baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build contrastive pair matrix\n",
    "contrastive_vectors = []\n",
    "contrastive_labels = []\n",
    "contrastive_categories = []\n",
    "\n",
    "label_to_idx = {lab: i for i, lab in enumerate(labels)}\n",
    "\n",
    "for cat in unique_cats:\n",
    "    # Get sorted members of this category\n",
    "    members = sorted([lab for lab in labels if lab.split('_')[0] == cat])\n",
    "    \n",
    "    # All ordered pairs (combinations)\n",
    "    for a, b in combinations(members, 2):\n",
    "        idx_a = label_to_idx[a]\n",
    "        idx_b = label_to_idx[b]\n",
    "        v_contrast = X[idx_a] - X[idx_b]\n",
    "        \n",
    "        short_a = a.split('_', 1)[1][:20]\n",
    "        short_b = b.split('_', 1)[1][:20]\n",
    "        pair_label = f\"{cat}: {short_a} vs {short_b}\"\n",
    "        \n",
    "        contrastive_vectors.append(v_contrast)\n",
    "        contrastive_labels.append(pair_label)\n",
    "        contrastive_categories.append(cat)\n",
    "\n",
    "X_contrastive = np.array(contrastive_vectors)\n",
    "\n",
    "print(f\"Contrastive matrix shape: {X_contrastive.shape}\")\n",
    "print(f\"Total contrastive pairs: {len(contrastive_labels)}\")\n",
    "print(f\"\\nPairs per category:\")\n",
    "for cat in unique_cats:\n",
    "    n_pairs = sum(1 for c in contrastive_categories if c == cat)\n",
    "    n_members = sum(1 for c in categories if c == cat)\n",
    "    print(f\"  {cat:15s}: {n_members} members -> C({n_members},2) = {n_pairs} pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on contrastive matrix\n",
    "n_comp_contrast = min(30, len(contrastive_labels))\n",
    "pca_contrast = PCA(n_components=n_comp_contrast)\n",
    "X_pca_contrast = pca_contrast.fit_transform(X_contrastive)\n",
    "\n",
    "# Scree plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(range(1, 21), pca_contrast.explained_variance_ratio_[:20], color='steelblue', alpha=0.8)\n",
    "axes[0].set_xlabel('Principal Component', fontsize=12)\n",
    "axes[0].set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "axes[0].set_title('Scree Plot (contrastive pairs)', fontsize=13)\n",
    "axes[0].set_xticks(range(1, 21))\n",
    "\n",
    "cum_var_k = np.cumsum(pca_contrast.explained_variance_ratio_[:20])\n",
    "axes[1].plot(range(1, 21), cum_var_k, 'bo-', linewidth=2, markersize=6)\n",
    "axes[1].axhline(0.8, color='red', linestyle='--', alpha=0.5, label='80%')\n",
    "axes[1].axhline(0.9, color='orange', linestyle='--', alpha=0.5, label='90%')\n",
    "axes[1].set_xlabel('Number of Components', fontsize=12)\n",
    "axes[1].set_ylabel('Cumulative Explained Variance', fontsize=12)\n",
    "axes[1].set_title('Cumulative Variance (contrastive)', fontsize=13)\n",
    "axes[1].legend()\n",
    "axes[1].set_xticks(range(1, 21))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop 10 PCs explained variance (contrastive):\")\n",
    "for i in range(10):\n",
    "    print(f\"  PC{i+1}: {pca_contrast.explained_variance_ratio_[i]:.4f} \"\n",
    "          f\"(cumulative: {sum(pca_contrast.explained_variance_ratio_[:i+1]):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive PCA: PC1 vs PC2 scatter\n",
    "df_contrast = pd.DataFrame({\n",
    "    'PC1': X_pca_contrast[:, 0],\n",
    "    'PC2': X_pca_contrast[:, 1],\n",
    "    'label': contrastive_labels,\n",
    "    'category': contrastive_categories,\n",
    "})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(18, 14))\n",
    "\n",
    "for cat in unique_cats:\n",
    "    mask = df_contrast['category'] == cat\n",
    "    if mask.any():\n",
    "        ax.scatter(\n",
    "            df_contrast.loc[mask, 'PC1'], df_contrast.loc[mask, 'PC2'],\n",
    "            label=cat, color=cat_to_color[cat], s=60, alpha=0.7,\n",
    "            edgecolors='white', linewidth=0.5,\n",
    "        )\n",
    "\n",
    "# Annotate politically relevant pairs\n",
    "political_keywords = [\n",
    "    'Democrat', 'Republican', 'Liberal', 'Conservative',\n",
    "    'gun owner', 'believes in God', 'does not believe',\n",
    "    'rural', 'central city',\n",
    "]\n",
    "for _, row in df_contrast.iterrows():\n",
    "    if any(kw in row['label'] for kw in political_keywords):\n",
    "        ax.annotate(\n",
    "            row['label'], (row['PC1'], row['PC2']),\n",
    "            fontsize=4.5, alpha=0.7,\n",
    "            xytext=(3, 3), textcoords='offset points',\n",
    "        )\n",
    "\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.2)\n",
    "ax.axvline(0, color='black', linestyle='--', alpha=0.2)\n",
    "ax.set_xlabel(f'PC1 ({pca_contrast.explained_variance_ratio_[0]:.1%} variance)', fontsize=14)\n",
    "ax.set_ylabel(f'PC2 ({pca_contrast.explained_variance_ratio_[1]:.1%} variance)', fontsize=14)\n",
    "ax.set_title(f'Contrastive Demographic Space: PC1 vs PC2 (Layer {REF_LAYER})', fontsize=16)\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8, ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive PCA: PC Poles (top/bottom 10 on PC1-PC5)\n",
    "print(\"CONTRASTIVE PCA — PC Poles\")\n",
    "for pc_idx in range(5):\n",
    "    pc_name = f'PC{pc_idx + 1}'\n",
    "    projections = X_pca_contrast[:, pc_idx]\n",
    "    sorted_idx = np.argsort(projections)\n",
    "\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"{pc_name} — Explained Variance: {pca_contrast.explained_variance_ratio_[pc_idx]:.2%}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "\n",
    "    print(f\"\\n  POSITIVE pole (top 10):\")\n",
    "    for i in sorted_idx[-10:][::-1]:\n",
    "        print(f\"    {projections[i]:+8.4f}  [{contrastive_categories[i]:12s}]  {contrastive_labels[i]}\")\n",
    "\n",
    "    print(f\"\\n  NEGATIVE pole (bottom 10):\")\n",
    "    for i in sorted_idx[:10]:\n",
    "        print(f\"    {projections[i]:+8.4f}  [{contrastive_categories[i]:12s}]  {contrastive_labels[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side-by-Side Comparison: Original vs Centered vs Contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three-panel figure: Original, Centered, Contrastive — all PC1 vs PC2\n",
    "fig, axes = plt.subplots(1, 3, figsize=(24, 8))\n",
    "\n",
    "# Panel 1: Original PCA\n",
    "ax = axes[0]\n",
    "for cat in unique_cats:\n",
    "    mask = [c == cat for c in categories]\n",
    "    indices = [i for i, m in enumerate(mask) if m]\n",
    "    ax.scatter(\n",
    "        X_pca_orig[indices, 0], X_pca_orig[indices, 1],\n",
    "        label=cat, color=cat_to_color[cat], s=50, alpha=0.7,\n",
    "        edgecolors='white', linewidth=0.3,\n",
    "    )\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.2)\n",
    "ax.axvline(0, color='black', linestyle='--', alpha=0.2)\n",
    "ax.set_xlabel(f\"PC1 ({pca_orig.explained_variance_ratio_[0]:.1%})\")\n",
    "ax.set_ylabel(f\"PC2 ({pca_orig.explained_variance_ratio_[1]:.1%})\")\n",
    "ax.set_title('Original (X vs generic)', fontsize=14)\n",
    "\n",
    "# Panel 2: Centered PCA\n",
    "ax = axes[1]\n",
    "for cat in unique_cats:\n",
    "    mask = [c == cat for c in categories]\n",
    "    indices = [i for i, m in enumerate(mask) if m]\n",
    "    ax.scatter(\n",
    "        X_pca_centered[indices, 0], X_pca_centered[indices, 1],\n",
    "        label=cat, color=cat_to_color[cat], s=50, alpha=0.7,\n",
    "        edgecolors='white', linewidth=0.3,\n",
    "    )\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.2)\n",
    "ax.axvline(0, color='black', linestyle='--', alpha=0.2)\n",
    "ax.set_xlabel(f\"PC1 ({pca_centered.explained_variance_ratio_[0]:.1%})\")\n",
    "ax.set_ylabel(f\"PC2 ({pca_centered.explained_variance_ratio_[1]:.1%})\")\n",
    "ax.set_title('Within-Category Centered', fontsize=14)\n",
    "\n",
    "# Panel 3: Contrastive PCA\n",
    "ax = axes[2]\n",
    "for cat in unique_cats:\n",
    "    mask = [c == cat for c in contrastive_categories]\n",
    "    if any(mask):\n",
    "        indices = [i for i, m in enumerate(mask) if m]\n",
    "        ax.scatter(\n",
    "            X_pca_contrast[indices, 0], X_pca_contrast[indices, 1],\n",
    "            label=cat, color=cat_to_color[cat], s=40, alpha=0.7,\n",
    "            edgecolors='white', linewidth=0.3,\n",
    "        )\n",
    "ax.axhline(0, color='black', linestyle='--', alpha=0.2)\n",
    "ax.axvline(0, color='black', linestyle='--', alpha=0.2)\n",
    "ax.set_xlabel(f\"PC1 ({pca_contrast.explained_variance_ratio_[0]:.1%})\")\n",
    "ax.set_ylabel(f\"PC2 ({pca_contrast.explained_variance_ratio_[1]:.1%})\")\n",
    "ax.set_title('Explicit Contrastive Pairs', fontsize=14)\n",
    "\n",
    "axes[-1].legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=6, ncol=2)\n",
    "plt.suptitle(f'Three PCA Approaches Compared (Layer {REF_LAYER})', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print comparison table\n",
    "print(f\"\\nExplained Variance Comparison:\")\n",
    "print(f\"{'PC':>4s}  {'Original':>10s}  {'Centered':>10s}  {'Contrastive':>12s}\")\n",
    "print(\"-\" * 42)\n",
    "for i in range(10):\n",
    "    print(f\"PC{i+1:>2d}  {pca_orig.explained_variance_ratio_[i]:10.4f}  \"\n",
    "          f\"{pca_centered.explained_variance_ratio_[i]:10.4f}  \"\n",
    "          f\"{pca_contrast.explained_variance_ratio_[i]:12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liberal-Conservative Hypothesis Test\n",
    "\n",
    "For each PCA space, compute mean PC1 for liberal vs conservative clusters\n",
    "and culture-war demographics. Check which approach best separates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define clusters\n",
    "liberal_labels = [\n",
    "    'PartyID_Strong Democrat',\n",
    "    'PolViews_person with a liberal political view',\n",
    "    'PolViews_person with an extremely liberal political view',\n",
    "]\n",
    "conservative_labels = [\n",
    "    'PartyID_Strong Republican',\n",
    "    'PolViews_person with a conservative political view',\n",
    "    'PolViews_person with an extremely conservative political view',\n",
    "]\n",
    "culture_war = {\n",
    "    'Gun owner vs Non-owner': ('OwnGun_gun owner', 'OwnGun_person who does not own a gun'),\n",
    "    'Firm believer vs Non-believer': ('God_person who firmly believes in God', 'God_person who does not believe in God'),\n",
    "    'Rural vs Urban': ('Urbanity_person living in a rural area', 'Urbanity_person living in a central city'),\n",
    "}\n",
    "\n",
    "def get_mean_pc1(pca_result, target_labels, all_labels):\n",
    "    \"\"\"Get mean PC1 projection for a set of labels.\"\"\"\n",
    "    vals = []\n",
    "    for lab in target_labels:\n",
    "        if lab in all_labels:\n",
    "            idx = all_labels.index(lab)\n",
    "            vals.append(pca_result[idx, 0])\n",
    "    return np.mean(vals) if vals else np.nan\n",
    "\n",
    "# --- Original PCA ---\n",
    "print(\"LIBERAL-CONSERVATIVE SEPARATION ON PC1\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "approaches = [\n",
    "    ('Original', X_pca_orig, labels),\n",
    "    ('Centered', X_pca_centered, labels),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, pca_result, lab_list in approaches:\n",
    "    lib_mean = get_mean_pc1(pca_result, liberal_labels, lab_list)\n",
    "    con_mean = get_mean_pc1(pca_result, conservative_labels, lab_list)\n",
    "    separation = abs(lib_mean - con_mean)\n",
    "    results.append({'Approach': name, 'Liberal mean PC1': lib_mean,\n",
    "                    'Conservative mean PC1': con_mean, 'Separation': separation})\n",
    "\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Liberal cluster mean PC1:      {lib_mean:+.4f}\")\n",
    "    print(f\"  Conservative cluster mean PC1: {con_mean:+.4f}\")\n",
    "    print(f\"  Separation (|diff|):           {separation:.4f}\")\n",
    "\n",
    "    print(f\"\\n  Culture war demographics on PC1:\")\n",
    "    for desc, (lab_a, lab_b) in culture_war.items():\n",
    "        if lab_a in lab_list and lab_b in lab_list:\n",
    "            idx_a = lab_list.index(lab_a)\n",
    "            idx_b = lab_list.index(lab_b)\n",
    "            val_a = pca_result[idx_a, 0]\n",
    "            val_b = pca_result[idx_b, 0]\n",
    "            print(f\"    {desc:35s}: {val_a:+.4f} vs {val_b:+.4f}  (diff={val_a - val_b:+.4f})\")\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(\"\\nSummary Table:\")\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSS Phi-Coefficient Validation\n",
    "\n",
    "Compare cosine similarity between centered vectors with GSS phi-coefficients.\n",
    "If centering removes shared baseline noise, the correlation should improve from the\n",
    "Phase 5 baseline (r~0.04)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GSS-to-vector mapping (same as Phase 5)\n",
    "GSS_TO_VECTOR = {\n",
    "    # Race\n",
    "    'race_Black': 'Race_Black person',\n",
    "    'race_White': 'Race_White person',\n",
    "    'race_Other': 'Race_person of different race than Black or White',\n",
    "    # PartyID\n",
    "    'partyid_Strong democrat': 'PartyID_Strong Democrat',\n",
    "    'partyid_Strong republican': 'PartyID_Strong Republican',\n",
    "    'partyid_Not very strong democrat': 'PartyID_Democrat',\n",
    "    'partyid_Not very strong republican': 'PartyID_Republican',\n",
    "    'partyid_Independent, close to democrat': 'PartyID_Independent leaning Democrat',\n",
    "    'partyid_Independent, close to republican': 'PartyID_Independent leaning Republican',\n",
    "    'partyid_Independent (neither, no response)': 'PartyID_Political Independent',\n",
    "    'partyid_Other party': 'PartyID_Other',\n",
    "    # Sex\n",
    "    'sex_MALE': 'Sex_man',\n",
    "    'sex_FEMALE': 'Sex_woman',\n",
    "    # Degree\n",
    "    'degree_Less than high school': 'Degree_person with less than high school education',\n",
    "    'degree_High school': 'Degree_high school graduate',\n",
    "    'degree_Associate/junior college': 'Degree_person with some college education',\n",
    "    \"degree_Bachelor's\": \"Degree_person with a bachelor's degree\",\n",
    "    'degree_Graduate': 'Degree_person with a graduate degree',\n",
    "    # Religion\n",
    "    'relig_Catholic': 'Religion_Catholic',\n",
    "    'relig_Protestant': 'Religion_Protestant',\n",
    "    'relig_Buddhism': 'Religion_Buddhist',\n",
    "    'relig_Jewish': 'Religion_Jewish person',\n",
    "    'relig_Muslim/Islam': 'Religion_Muslim',\n",
    "    'relig_Hinduism': 'Religion_Hindu',\n",
    "    'relig_Other': 'Religion_person of other religion',\n",
    "    # PolViews\n",
    "    'polviews_Extremely liberal': 'PolViews_person with an extremely liberal political view',\n",
    "    'polviews_Liberal': 'PolViews_person with a liberal political view',\n",
    "    'polviews_Slightly liberal': 'PolViews_person with a slightly liberal political view',\n",
    "    'polviews_Moderate, middle of the road': 'PolViews_person with a neutral political view',\n",
    "    'polviews_Slightly conservative': 'PolViews_person with a slightly conservative political view',\n",
    "    'polviews_Conservative': 'PolViews_person with a conservative political view',\n",
    "    'polviews_Extremely conservative': 'PolViews_person with an extremely conservative political view',\n",
    "    # Generation\n",
    "    'generation_Generation Z': 'Generation_person from Generation Z',\n",
    "    'generation_Millennial': 'Generation_Millennial',\n",
    "    'generation_Generation X': 'Generation_person from Generation X',\n",
    "    'generation_Baby Boomer': 'Generation_Baby Boomer',\n",
    "    'generation_Silent Generation': 'Generation_person from the Silent Generation',\n",
    "    # Marital\n",
    "    'marital_Married': 'Marital_married person',\n",
    "    'marital_Divorced': 'Marital_divorced person',\n",
    "    'marital_Widowed': 'Marital_widowed person',\n",
    "    'marital_Separated': 'Marital_separated person',\n",
    "    'marital_Never married': 'Marital_person who has never been married',\n",
    "    # Age\n",
    "    'age_group_Youth (15-24)': 'Age_young person aged 15 to 24',\n",
    "    'age_group_Young Adult (25-39)': 'Age_young adult aged 25 to 39',\n",
    "    'age_group_Middle Adult (40-64)': 'Age_middle-aged adult aged 40 to 64',\n",
    "    'age_group_Elderly (65+)': 'Age_elderly person aged 65 or older',\n",
    "    # Children\n",
    "    'childs_group_No children': 'Children_person with no children',\n",
    "    'childs_group_1-2 children': 'Children_person with one or two children',\n",
    "    'childs_group_3 or more children': 'Children_person with three or more children',\n",
    "    # Immigration\n",
    "    'immig_gen_1st generation immigrant': 'ImmigGen_first-generation immigrant to the United States',\n",
    "    'immig_gen_2nd generation immigrant': 'ImmigGen_second-generation immigrant in the United States',\n",
    "    'immig_gen_3rd+ generation American': 'ImmigGen_third-generation or later American',\n",
    "    # Region (grew up)\n",
    "    'reg16_Northeast': 'Region16_person who grew up in the Northeast',\n",
    "    'reg16_Midwest': 'Region16_person who grew up in the Midwest',\n",
    "    'reg16_South': 'Region16_person who grew up in the South',\n",
    "    'reg16_West': 'Region16_person who grew up in the West',\n",
    "    'reg16_Foreign': 'Region16_person who grew up in a foreign country',\n",
    "    # Family income\n",
    "    'income_group_Income under $10k': 'FamilyIncome_person with a family income under $10,000',\n",
    "    'income_group_Income $10k-$25k': 'FamilyIncome_person with a family income between $10,000 and $25,000',\n",
    "    'income_group_Income $25k or more': 'FamilyIncome_person with a family income of $25,000 or more',\n",
    "    # Region (current)\n",
    "    'region_Northeast': 'Region_person living in the Northeast',\n",
    "    'region_Midwest': 'Region_person living in the Midwest',\n",
    "    'region_South': 'Region_person living in the South',\n",
    "    'region_West': 'Region_person living in the West',\n",
    "    # Urbanity\n",
    "    'srcbelt_group_Central city': 'Urbanity_person living in a central city',\n",
    "    'srcbelt_group_Suburban': 'Urbanity_person living in the suburbs',\n",
    "    'srcbelt_group_Other urban': 'Urbanity_person living in a small city or town',\n",
    "    'srcbelt_group_Rural': 'Urbanity_person living in a rural area',\n",
    "    # Happiness\n",
    "    'happy_Very happy': 'Happy_very happy person',\n",
    "    'happy_Pretty happy': 'Happy_fairly happy person',\n",
    "    'happy_Not too happy': 'Happy_unhappy person',\n",
    "    # Health\n",
    "    'health_Excellent': 'Health_person in excellent health',\n",
    "    'health_Good': 'Health_person in good health',\n",
    "    'health_Fair': 'Health_person in fair health',\n",
    "    'health_Poor': 'Health_person in poor health',\n",
    "    # Life excitement\n",
    "    'life_Exciting': 'Life_person who finds life exciting',\n",
    "    'life_Routine': 'Life_person who finds life routine',\n",
    "    'life_Dull': 'Life_person who finds life dull',\n",
    "    # Job satisfaction\n",
    "    'satjob_group_Very satisfied': 'SatJob_person very satisfied with their job',\n",
    "    'satjob_group_Moderately satisfied': 'SatJob_person moderately satisfied with their job',\n",
    "    'satjob_group_Dissatisfied': 'SatJob_person dissatisfied with their job',\n",
    "    # Social class\n",
    "    'class__Lower class': 'Class_lower class person',\n",
    "    'class__Working class': 'Class_working class person',\n",
    "    'class__Middle class': 'Class_middle class person',\n",
    "    'class__Upper class': 'Class_upper class person',\n",
    "    # Financial satisfaction\n",
    "    'satfin_Pretty well satisfied': 'SatFin_person satisfied with their financial situation',\n",
    "    'satfin_More or less satisfied': 'SatFin_person somewhat satisfied with their financial situation',\n",
    "    'satfin_Not satisfied at all': 'SatFin_person not satisfied with their financial situation',\n",
    "    # Gun ownership\n",
    "    'owngun_YES': 'OwnGun_gun owner',\n",
    "    'owngun_NO': 'OwnGun_person who does not own a gun',\n",
    "    # Belief in God\n",
    "    'god_No doubts': 'God_person who firmly believes in God',\n",
    "    'god_Believe with doubts': 'God_person who believes in God but with some doubts',\n",
    "    'god_Believe sometimes': 'God_person who sometimes believes in God',\n",
    "    \"god_Don't know, no way to find out\": 'God_agnostic person',\n",
    "    'god_Higher power': 'God_person who believes in a higher power but not a personal God',\n",
    "    \"god_Don't believe\": 'God_person who does not believe in God',\n",
    "    # Occupation\n",
    "    'occ_group_Service': 'Occupation_person working in a service occupation',\n",
    "    'occ_group_Natural Resources, Construction, and Maintenance': 'Occupation_person working in construction, maintenance, or natural resources',\n",
    "    'occ_group_Office and Administrative Support': 'Occupation_person working in an office or administrative support role',\n",
    "    'occ_group_Management, Business, and Financial': 'Occupation_person working in management, business, or finance',\n",
    "    'occ_group_Education, Legal, Community Service, Arts, and Media': 'Occupation_person working in education, law, community service, arts, or media',\n",
    "    'occ_group_Computer, Engineering, and Science': 'Occupation_person working in computer science, engineering, or science',\n",
    "    'occ_group_Military': 'Occupation_person serving in the military',\n",
    "    'occ_group_Production, Transportation, and Material Moving': 'Occupation_person working in production or transportation',\n",
    "    'occ_group_Sales and Related': 'Occupation_person working in sales',\n",
    "    'occ_group_Healthcare Practitioners and Technical': 'Occupation_healthcare practitioner',\n",
    "    # Industry\n",
    "    'indus_group_Health Care and Social Assistance': 'Industry_person working in healthcare or social assistance',\n",
    "    'indus_group_Wholesale Trade': 'Industry_person working in wholesale trade',\n",
    "    'indus_group_Manufacturing': 'Industry_person working in manufacturing',\n",
    "    'indus_group_Construction': 'Industry_person working in the construction industry',\n",
    "    'indus_group_Finance and Insurance, and Real Estate': 'Industry_person working in finance, insurance, or real estate',\n",
    "    'indus_group_Arts, Entertainment, Recreation, and Accommodation and Food Services': 'Industry_person working in arts, entertainment, or food services',\n",
    "    'indus_group_Information': 'Industry_person working in the information industry',\n",
    "    'indus_group_Educational Services': 'Industry_person working in educational services',\n",
    "    'indus_group_Public Administration': 'Industry_person working in public administration',\n",
    "    'indus_group_Professional, Scientific, Management, and Administrative Services': 'Industry_person working in professional or scientific services',\n",
    "    'indus_group_Transportation and Warehousing, and Utilities': 'Industry_person working in transportation or utilities',\n",
    "    'indus_group_Retail Trade': 'Industry_person working in retail',\n",
    "    'indus_group_Other Services (Except Public Administration)': 'Industry_person working in other services',\n",
    "    'indus_group_Agriculture, Forestry, Fishing and Hunting, and Mining': 'Industry_person working in agriculture or mining',\n",
    "}\n",
    "\n",
    "print(f\"GSS_TO_VECTOR mapping: {len(GSS_TO_VECTOR)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GSS phi-coefficient correlations\n",
    "corr_df = pd.read_csv(os.path.join(DATA_DIR, \"gss_correlation_pairs.csv\"))\n",
    "print(f\"Loaded {len(corr_df)} correlation pairs\")\n",
    "\n",
    "# Compute cosine similarities for original and centered vectors\n",
    "cos_sim_orig = cosine_similarity(X)\n",
    "cos_sim_centered = cosine_similarity(X_centered)\n",
    "\n",
    "# Build comparison dataframe\n",
    "comparison_data = []\n",
    "\n",
    "for _, row in corr_df.iterrows():\n",
    "    var1, var2, phi = row['Var1'], row['Var2'], row['Correlation']\n",
    "\n",
    "    vec_label1 = GSS_TO_VECTOR.get(var1)\n",
    "    vec_label2 = GSS_TO_VECTOR.get(var2)\n",
    "\n",
    "    if vec_label1 and vec_label2 and vec_label1 in label_to_idx and vec_label2 in label_to_idx:\n",
    "        idx1 = label_to_idx[vec_label1]\n",
    "        idx2 = label_to_idx[vec_label2]\n",
    "        comparison_data.append({\n",
    "            'gss_var1': var1, 'gss_var2': var2,\n",
    "            'vec_label1': vec_label1, 'vec_label2': vec_label2,\n",
    "            'phi_coefficient': phi,\n",
    "            'cos_sim_original': cos_sim_orig[idx1, idx2],\n",
    "            'cos_sim_centered': cos_sim_centered[idx1, idx2],\n",
    "        })\n",
    "\n",
    "comp_df = pd.DataFrame(comparison_data)\n",
    "print(f\"Matched {len(comp_df)} out of {len(corr_df)} correlation pairs\")\n",
    "\n",
    "# Compute correlations\n",
    "pearson_orig = comp_df['phi_coefficient'].corr(comp_df['cos_sim_original'])\n",
    "spearman_orig = comp_df['phi_coefficient'].corr(comp_df['cos_sim_original'], method='spearman')\n",
    "pearson_centered = comp_df['phi_coefficient'].corr(comp_df['cos_sim_centered'])\n",
    "spearman_centered = comp_df['phi_coefficient'].corr(comp_df['cos_sim_centered'], method='spearman')\n",
    "\n",
    "print(f\"\\nGSS Validation Results:\")\n",
    "print(f\"{'Approach':>15s}  {'Pearson r':>10s}  {'Spearman rho':>13s}\")\n",
    "print(\"-\" * 42)\n",
    "print(f\"{'Original':>15s}  {pearson_orig:10.4f}  {spearman_orig:13.4f}\")\n",
    "print(f\"{'Centered':>15s}  {pearson_centered:10.4f}  {spearman_centered:13.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots: phi-coefficient vs cosine similarity (original vs centered, side by side)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "for ax, col, title, r_val in [\n",
    "    (axes[0], 'cos_sim_original', 'Original Vectors', pearson_orig),\n",
    "    (axes[1], 'cos_sim_centered', 'Centered Vectors', pearson_centered),\n",
    "]:\n",
    "    ax.scatter(comp_df['phi_coefficient'], comp_df[col],\n",
    "              alpha=0.3, s=20, color='steelblue')\n",
    "\n",
    "    # Fit line\n",
    "    z = np.polyfit(comp_df['phi_coefficient'], comp_df[col], 1)\n",
    "    p = np.poly1d(z)\n",
    "    x_fit = np.linspace(comp_df['phi_coefficient'].min(), comp_df['phi_coefficient'].max(), 100)\n",
    "    ax.plot(x_fit, p(x_fit), 'r-', linewidth=2, alpha=0.7)\n",
    "\n",
    "    ax.set_xlabel('GSS Phi-Coefficient', fontsize=12)\n",
    "    ax.set_ylabel('Vector Cosine Similarity', fontsize=12)\n",
    "    ax.set_title(f'{title} (r={r_val:.3f})', fontsize=13)\n",
    "    ax.axhline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "    ax.axvline(0, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.suptitle('GSS Phi-Coefficient vs Vector Cosine Similarity', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show cross-category vs within-category breakdown\n",
    "comp_df['same_category'] = [\n",
    "    row['vec_label1'].split('_')[0] == row['vec_label2'].split('_')[0]\n",
    "    for _, row in comp_df.iterrows()\n",
    "]\n",
    "\n",
    "print(f\"\\nBreakdown by pair type:\")\n",
    "print(f\"{'Type':>20s}  {'N':>5s}  {'Pearson (orig)':>15s}  {'Pearson (cent)':>15s}\")\n",
    "print(\"-\" * 60)\n",
    "for same_cat, label in [(True, 'Within-category'), (False, 'Cross-category')]:\n",
    "    sub = comp_df[comp_df['same_category'] == same_cat]\n",
    "    if len(sub) > 2:\n",
    "        r_o = sub['phi_coefficient'].corr(sub['cos_sim_original'])\n",
    "        r_c = sub['phi_coefficient'].corr(sub['cos_sim_centered'])\n",
    "        print(f\"{label:>20s}  {len(sub):5d}  {r_o:15.4f}  {r_c:15.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layer Comparison (Centered PCA)\n",
    "\n",
    "Run within-category centering + PCA across layers 5, 9, 13, 17, 20 to check\n",
    "whether centering improves cross-layer consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_layers = [5, 9, 13, 17, 20]\n",
    "layer_results_orig = {}\n",
    "layer_results_centered = {}\n",
    "\n",
    "for layer in comparison_layers:\n",
    "    vecs = torch.load(\n",
    "        os.path.join(VECTOR_DIR, f\"gss_demographic_vectors_layer{layer}.pt\"),\n",
    "        map_location=\"cpu\",\n",
    "        weights_only=True,\n",
    "    )\n",
    "\n",
    "    layer_labels = sorted(vecs.keys())\n",
    "    layer_cats = [lab.split('_')[0] for lab in layer_labels]\n",
    "    layer_X = np.array([\n",
    "        (vecs[lab]['vector'] * vecs[lab]['magnitude']).numpy()\n",
    "        for lab in layer_labels\n",
    "    ])\n",
    "\n",
    "    # Original PCA\n",
    "    pca_o = PCA(n_components=min(20, len(layer_labels)))\n",
    "    X_pca_o = pca_o.fit_transform(layer_X)\n",
    "    layer_results_orig[layer] = {\n",
    "        'labels': layer_labels, 'categories': layer_cats,\n",
    "        'X_pca': X_pca_o, 'pca': pca_o,\n",
    "        'explained_variance': pca_o.explained_variance_ratio_,\n",
    "    }\n",
    "\n",
    "    # Within-category centering\n",
    "    layer_X_c = np.copy(layer_X)\n",
    "    layer_unique_cats = sorted(set(layer_cats))\n",
    "    for cat in layer_unique_cats:\n",
    "        mask = [c == cat for c in layer_cats]\n",
    "        indices = [i for i, m in enumerate(mask) if m]\n",
    "        cat_mean = layer_X[indices].mean(axis=0)\n",
    "        layer_X_c[indices] -= cat_mean\n",
    "\n",
    "    pca_c = PCA(n_components=min(20, len(layer_labels)))\n",
    "    X_pca_c = pca_c.fit_transform(layer_X_c)\n",
    "    layer_results_centered[layer] = {\n",
    "        'labels': layer_labels, 'categories': layer_cats,\n",
    "        'X_pca': X_pca_c, 'pca': pca_c,\n",
    "        'explained_variance': pca_c.explained_variance_ratio_,\n",
    "    }\n",
    "\n",
    "    print(f\"Layer {layer:2d}:  Original PC1={pca_o.explained_variance_ratio_[0]:.3f}  \"\n",
    "          f\"Centered PC1={pca_c.explained_variance_ratio_[0]:.3f}  \"\n",
    "          f\"Original top-3={sum(pca_o.explained_variance_ratio_[:3]):.3f}  \"\n",
    "          f\"Centered top-3={sum(pca_c.explained_variance_ratio_[:3]):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: explained variance comparison across layers (original vs centered)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# PC1-PC10: original (dashed) vs centered (solid)\n",
    "ax = axes[0]\n",
    "colors_layer = plt.cm.viridis(np.linspace(0, 1, len(comparison_layers)))\n",
    "for i, layer in enumerate(comparison_layers):\n",
    "    ev_o = layer_results_orig[layer]['explained_variance'][:10]\n",
    "    ev_c = layer_results_centered[layer]['explained_variance'][:10]\n",
    "    ax.plot(range(1, 11), ev_o, 'o--', color=colors_layer[i], alpha=0.5, linewidth=1, markersize=4)\n",
    "    ax.plot(range(1, 11), ev_c, 's-', color=colors_layer[i], label=f'Layer {layer}', linewidth=2, markersize=5)\n",
    "ax.set_xlabel('Principal Component', fontsize=12)\n",
    "ax.set_ylabel('Explained Variance Ratio', fontsize=12)\n",
    "ax.set_title('Centered (solid) vs Original (dashed)', fontsize=13)\n",
    "ax.legend()\n",
    "ax.set_xticks(range(1, 11))\n",
    "\n",
    "# Cumulative centered\n",
    "ax = axes[1]\n",
    "for i, layer in enumerate(comparison_layers):\n",
    "    ev_c = layer_results_centered[layer]['explained_variance'][:10]\n",
    "    ax.plot(range(1, 11), np.cumsum(ev_c), 'o-', color=colors_layer[i],\n",
    "            label=f'Layer {layer}', linewidth=2, markersize=6)\n",
    "ax.set_xlabel('Number of Components', fontsize=12)\n",
    "ax.set_ylabel('Cumulative Explained Variance', fontsize=12)\n",
    "ax.set_title('Cumulative Variance (Centered PCA)', fontsize=13)\n",
    "ax.legend()\n",
    "ax.set_xticks(range(1, 11))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare PC1 poles across layers (centered)\n",
    "print(\"\\nCentered PC1 Poles Across Layers\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for layer in comparison_layers:\n",
    "    r = layer_results_centered[layer]\n",
    "    projections = r['X_pca'][:, 0]\n",
    "    sorted_idx = np.argsort(projections)\n",
    "\n",
    "    top5 = [r['labels'][i].split('_', 1)[1][:30] for i in sorted_idx[-5:][::-1]]\n",
    "    bot5 = [r['labels'][i].split('_', 1)[1][:30] for i in sorted_idx[:5]]\n",
    "\n",
    "    print(f\"\\nLayer {layer} (PC1 = {r['explained_variance'][0]:.1%}):\")\n",
    "    print(f\"  + pole: {', '.join(top5)}\")\n",
    "    print(f\"  - pole: {', '.join(bot5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Interpretation\n",
    "\n",
    "### Motivation\n",
    "Phase 5 PCA on the original \"X vs. generic\" vectors was dominated by a shared baseline\n",
    "(mean cosine similarity ~0.39 across all pairs). PC1 (23.6%) captured \"attitudinal specificity\"\n",
    "rather than the expected liberal-conservative axis, and GSS phi-coefficient validation showed\n",
    "near-zero correlation (r~0.04).\n",
    "\n",
    "### Approach 2: Within-Category Centering\n",
    "- Subtracts the category mean from each vector, removing within-category shared variance\n",
    "- Preserves the 120-vector structure, so direct comparison with original PCA is possible\n",
    "- GSS validation uses cosine similarity on centered vectors\n",
    "\n",
    "### Approach 3: Explicit Contrastive Pairs\n",
    "- Computes all C(n,2) pairwise differences within each category (~317 vectors)\n",
    "- Each contrastive vector directly encodes \"A vs B\" with no shared baseline\n",
    "- Produces a larger matrix but with purely oppositional information\n",
    "\n",
    "### Key Questions\n",
    "1. Does PC1 now separate liberal from conservative after removing the shared baseline?\n",
    "2. Does GSS phi-coefficient validation improve from the r~0.04 baseline?\n",
    "3. Are the PC poles more stable across layers after centering?\n",
    "4. Which approach (centering vs contrastive) provides cleaner demographic axes?\n",
    "\n",
    "### Next Steps\n",
    "- **Phase 6**: LLM judge filtering for cleaner vector estimates\n",
    "- **Phase 7**: Multi-turn demographic drift tracking\n",
    "- **Phase 8**: Activation capping along demographic axes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}